## Ways to pretrain a model and best practices

### Research Papers

### Code Samples
1. [Pretrain a Bart from scratch](https://github.com/HSaurabh0919/CTransformers/blob/main/Pretrain/Code/bart_pretrain.py)
2. [BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)

Pretraining Document Transformations:

![](https://github.com/HSaurabh0919/CTransformers/blob/main/Pretrain/assets/Screenshot%20from%202023-04-04%2013-38-31.png)

