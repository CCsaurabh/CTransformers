{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqtwCIRZcq3xdtCYSuAbm7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "2bAub0JY-M8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYOFtp2v9mot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"epsil/Tinyllama-1.1B-miniguanaco\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"Are llm better than rule based chat?\"},\n",
        "]\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6IwXBo4-Di6",
        "outputId": "ef4c6642-5733-4435-842f-a1e9801cb1c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate</s>\n",
            "<|user|>\n",
            "Are llm better than rule based chat?</s>\n",
            "<|assistant|>\n",
            "LLM (Learned Language Models) and RBM (Recurrent Boltzmann Machines) are two machine learning algorithms that can be used to create chatbots. While LLM is designed to be more flexible and adaptable, RBM is a more traditional approach to training a chatbot. \n",
            "\n",
            "LLM is a type of deep learning algorithm that has shown great promise in natural language processing (NLP) and natural language generation (NLG). LLMs can learn to understand and generate natural-sounding responses to user queries. \n",
            "\n",
            "RBMs, on the other hand, are a type of recurrent neural network (RNN) that can be trained to generate responses to user queries. RBMs have been shown to be more flexible and adaptable than LLMs, allowing them to learn and adapt to changes in user queries over time. \n",
            "\n",
            "In general, RBMs are more versatile and capable of handling more complex natural language queries than LLMs. However, LLMs have been shown to be more effective for certain types of chatbot applications, such as those that require more complex language modeling or for those that require a more conversational or natural-s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zTZ1hyQ-2bj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}