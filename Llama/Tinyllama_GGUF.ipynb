{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMsDiJS9kBgIzniTx18S4oI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c060246673fb4bf2b1372637f03c8645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61b14c578d5a4d588321a470ae684f87",
              "IPY_MODEL_25b5e5ffbb0b431abe901c011b2eb82c",
              "IPY_MODEL_566e5a707e6741c29c3d55e1e17801e5"
            ],
            "layout": "IPY_MODEL_e7959e12a98c4942800a0d08ca20a1c4"
          }
        },
        "61b14c578d5a4d588321a470ae684f87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70a746fdcce14e7c98792f6246b5900e",
            "placeholder": "​",
            "style": "IPY_MODEL_b381db17a98a4e86802a117942cb874f",
            "value": "Fetching 0 files: "
          }
        },
        "25b5e5ffbb0b431abe901c011b2eb82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34018537c71b4f3b9c3981faadb4dd31",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a804f1cd730a46a69591daade8ee7171",
            "value": 0
          }
        },
        "566e5a707e6741c29c3d55e1e17801e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b1b8f34dca14a3ba48953e73c359dc9",
            "placeholder": "​",
            "style": "IPY_MODEL_96bc14f5b3ff4e4394e2f7d99d59af24",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "e7959e12a98c4942800a0d08ca20a1c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a746fdcce14e7c98792f6246b5900e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b381db17a98a4e86802a117942cb874f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34018537c71b4f3b9c3981faadb4dd31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a804f1cd730a46a69591daade8ee7171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b1b8f34dca14a3ba48953e73c359dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96bc14f5b3ff4e4394e2f7d99d59af24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fc3c232e4eb4b3dae9350b835948017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f5ea4dc8790427aa4928af3dc0585f0",
              "IPY_MODEL_854c5bcee18a44aebe5336e0953c789b",
              "IPY_MODEL_8dad9520383a435a81eea39b1af68606"
            ],
            "layout": "IPY_MODEL_81713d997e0b4d30880d9b0da0f91463"
          }
        },
        "4f5ea4dc8790427aa4928af3dc0585f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9a5f894679490793e7eac2f8148e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_5c231daf767946008ea00161f4aaee4c",
            "value": "Fetching 1 files: 100%"
          }
        },
        "854c5bcee18a44aebe5336e0953c789b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b25a00c4dbb84b2599e730dfdea97e50",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be214afa682544dfadbd1db0b8364c01",
            "value": 1
          }
        },
        "8dad9520383a435a81eea39b1af68606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a4bfbdcb4a412bafdb725ba481a727",
            "placeholder": "​",
            "style": "IPY_MODEL_f3b5af96bb5741e58f625bb90e6a6e2b",
            "value": " 1/1 [00:16&lt;00:00, 16.00s/it]"
          }
        },
        "81713d997e0b4d30880d9b0da0f91463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9a5f894679490793e7eac2f8148e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c231daf767946008ea00161f4aaee4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b25a00c4dbb84b2599e730dfdea97e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be214afa682544dfadbd1db0b8364c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77a4bfbdcb4a412bafdb725ba481a727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b5af96bb5741e58f625bb90e6a6e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f9bf24ca34646edbd25378d7ffb83dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24df9ca3bd2f43d181afc2ebc315bd28",
              "IPY_MODEL_20d4f15c1c004591a9ea326580ea1b56",
              "IPY_MODEL_6f11a8bd6b7447dd93108e0a4d7ee0e1"
            ],
            "layout": "IPY_MODEL_659cd6b71ffb487abc06cda577582779"
          }
        },
        "24df9ca3bd2f43d181afc2ebc315bd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa61c6e64dc445aa948a3c2d27305e3",
            "placeholder": "​",
            "style": "IPY_MODEL_676cecb1529d411fa502fe9113f9e122",
            "value": "Tinyllama-1b-v1.0.gguf: 100%"
          }
        },
        "20d4f15c1c004591a9ea326580ea1b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43f9f7029e694337898841411687ff81",
            "max": 1169808288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fc3df945bc449b58970e4a011f5c785",
            "value": 1169808288
          }
        },
        "6f11a8bd6b7447dd93108e0a4d7ee0e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6652ec1352744c6683ecce3b3f6064fa",
            "placeholder": "​",
            "style": "IPY_MODEL_01407200482e40eb8f4bd09821b240a6",
            "value": " 1.17G/1.17G [00:15&lt;00:00, 89.1MB/s]"
          }
        },
        "659cd6b71ffb487abc06cda577582779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa61c6e64dc445aa948a3c2d27305e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "676cecb1529d411fa502fe9113f9e122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43f9f7029e694337898841411687ff81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc3df945bc449b58970e4a011f5c785": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6652ec1352744c6683ecce3b3f6064fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01407200482e40eb8f4bd09821b240a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHdcpxUhyVey"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download"
      ],
      "metadata": {
        "id": "zYn5-DZKyahW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "snapshot_download(repo_id=model_id, local_dir=\"Tinyllama-hf\",\n",
        "                  local_dir_use_symlinks=False, revision=\"main\")"
      ],
      "metadata": {
        "id": "DA1ZQ9rNyfbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lash Tinyllama-hf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_I5jKAvysE1",
        "outputId": "9f38c501-5979-48a9-df23-3f66638a5250"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.1G\n",
            "4.0K drwxr-xr-x 2 root root 4.0K Jan 14 16:25 .\n",
            "4.0K drwxr-xr-x 1 root root 4.0K Jan 14 16:25 ..\n",
            "4.0K -rw-r--r-- 1 root root  608 Jan 14 16:25 config.json\n",
            "4.0K -rw-r--r-- 1 root root  566 Jan 14 16:25 eval_results.json\n",
            "4.0K -rw-r--r-- 1 root root  124 Jan 14 16:25 generation_config.json\n",
            "4.0K -rw-r--r-- 1 root root 1.5K Jan 14 16:25 .gitattributes\n",
            "2.1G -rw-r--r-- 1 root root 2.1G Jan 14 16:25 model.safetensors\n",
            "4.0K -rw-r--r-- 1 root root 3.1K Jan 14 16:25 README.md\n",
            "4.0K -rw-r--r-- 1 root root  551 Jan 14 16:25 special_tokens_map.json\n",
            "4.0K -rw-r--r-- 1 root root 1.3K Jan 14 16:25 tokenizer_config.json\n",
            "1.8M -rw-r--r-- 1 root root 1.8M Jan 14 16:25 tokenizer.json\n",
            "492K -rw-r--r-- 1 root root 489K Jan 14 16:25 tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "38uyC2FNy1mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Need to restart the system after this\n",
        "!pip install -r llama.cpp/requirements.txt\n"
      ],
      "metadata": {
        "id": "IYpem9Rey6d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9WvORcGy_v-",
        "outputId": "edfde0ac-9cf6-4a54-e549-c63ac840e725"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
            "/content/llama.cpp/gguf-py\n",
            "usage: convert.py [-h] [--awq-path AWQ_PATH] [--dump] [--dump-single] [--vocab-only]\n",
            "                  [--outtype {f32,f16,q8_0}] [--vocab-dir VOCAB_DIR] [--vocab-type {spm,bpe,hfft}]\n",
            "                  [--pad-vocab] [--outfile OUTFILE] [--ctx CTX] [--concurrency CONCURRENCY]\n",
            "                  [--big-endian]\n",
            "                  model\n",
            "\n",
            "Convert a LLaMa model to a GGML compatible file\n",
            "\n",
            "positional arguments:\n",
            "  model                 Directory containing the model file or the model file itself (*.pth, *.pt,\n",
            "                        *.bin)\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --awq-path AWQ_PATH   Path to the Activation-aware Weight Quantization cache file\n",
            "  --dump                Display the model content without converting it\n",
            "  --dump-single         Display the content of a single model file without conversion\n",
            "  --vocab-only          Extract and output only the vocabulary\n",
            "  --outtype {f32,f16,q8_0}\n",
            "                        Output format - note: q8_0 may be very slow (default: f16 or f32 based on\n",
            "                        input)\n",
            "  --vocab-dir VOCAB_DIR\n",
            "                        Directory containing the tokenizer.model, if separate from the model file\n",
            "  --vocab-type {spm,bpe,hfft}\n",
            "                        The vocabulary format used to define the tokenizer model (default: spm)\n",
            "  --pad-vocab           Add padding tokens when the model's vocabulary size exceeds the tokenizer\n",
            "                        metadata\n",
            "  --outfile OUTFILE     Specify the path for the output file (default is based on input)\n",
            "  --ctx CTX             Model training context (default is based on input)\n",
            "  --concurrency CONCURRENCY\n",
            "                        Concurrency used for conversion (default: 8)\n",
            "  --big-endian          Indicate that the model is executed on a big-endian machine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert.py Tinyllama-hf \\\n",
        "  --outfile Tinyllama-1b-v1.0.gguf \\\n",
        "  --outtype q8_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qqkfUT7ztrb",
        "outputId": "941e75bf-6ce9-467e-c1b4-e82124bdcf24"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/gguf-py\n",
            "Loading model file Tinyllama-hf/model.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, f_norm_eps=1e-05, n_experts=None, n_experts_used=None, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('Tinyllama-hf'))\n",
            "Loading vocab file 'Tinyllama-hf/tokenizer.model', type 'spm'\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 2048]\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 2048]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.norm.weight                                -> output_norm.weight                       | BF16   | [2048]\n",
            "Writing Tinyllama-1b-v1.0.gguf, format 7\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 2\n",
            "gguf: Setting chat_template to {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'system' %}\n",
            "{{ '<|system|>\n",
            "' + message['content'] + eos_token }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "[  1/201] Writing tensor output.weight                          | size  32000 x   2048  | type Q8_0 | T+   9\n",
            "[  2/201] Writing tensor token_embd.weight                      | size  32000 x   2048  | type Q8_0 | T+  10\n",
            "[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  10\n",
            "[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  10\n",
            "[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  10\n",
            "[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+  10\n",
            "[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  10\n",
            "[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  10\n",
            "[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  11\n",
            "[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  11\n",
            "[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  12\n",
            "[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  13\n",
            "[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  13\n",
            "[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+  13\n",
            "[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  13\n",
            "[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  13\n",
            "[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  14\n",
            "[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  14\n",
            "[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+  14\n",
            "[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  15\n",
            "[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  16\n",
            "[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  16\n",
            "[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+  17\n",
            "[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  17\n",
            "[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  17\n",
            "[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  17\n",
            "[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  17\n",
            "[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+  17\n",
            "[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  18\n",
            "[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  18\n",
            "[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  19\n",
            "[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+  19\n",
            "[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  19\n",
            "[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  19\n",
            "[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  19\n",
            "[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  19\n",
            "[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+  19\n",
            "[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  20\n",
            "[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  20\n",
            "[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  21\n",
            "[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  21\n",
            "[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  21\n",
            "[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  21\n",
            "[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  21\n",
            "[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+  21\n",
            "[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  22\n",
            "[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  23\n",
            "[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  23\n",
            "[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+  23\n",
            "[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  23\n",
            "[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  23\n",
            "[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  23\n",
            "[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  23\n",
            "[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  23\n",
            "[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  24\n",
            "[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  25\n",
            "[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  25\n",
            "[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  25\n",
            "[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  25\n",
            "[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  25\n",
            "[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  25\n",
            "[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  25\n",
            "[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  25\n",
            "[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  26\n",
            "[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  27\n",
            "[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  28\n",
            "[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  28\n",
            "[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  28\n",
            "[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  28\n",
            "[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  28\n",
            "[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  28\n",
            "[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  28\n",
            "[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  30\n",
            "[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  31\n",
            "[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  31\n",
            "[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  31\n",
            "[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  31\n",
            "[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  31\n",
            "[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  31\n",
            "[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  31\n",
            "[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  31\n",
            "[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  33\n",
            "[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  33\n",
            "[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  33\n",
            "[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  33\n",
            "[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  33\n",
            "[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  33\n",
            "[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  34\n",
            "[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  34\n",
            "[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  34\n",
            "[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  35\n",
            "[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  35\n",
            "[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  35\n",
            "[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  36\n",
            "[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  36\n",
            "[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  36\n",
            "[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  36\n",
            "[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  36\n",
            "[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  36\n",
            "[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  37\n",
            "[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  37\n",
            "[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  38\n",
            "[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  38\n",
            "[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  38\n",
            "[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  38\n",
            "[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  38\n",
            "[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  38\n",
            "[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  38\n",
            "[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  39\n",
            "[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  39\n",
            "[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  40\n",
            "[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  40\n",
            "[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  40\n",
            "[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  40\n",
            "[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  40\n",
            "[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  40\n",
            "[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  40\n",
            "[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  41\n",
            "[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  42\n",
            "[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  42\n",
            "[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  42\n",
            "[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  42\n",
            "[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  42\n",
            "[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  42\n",
            "[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  43\n",
            "[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  43\n",
            "[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type Q8_0 | T+  43\n",
            "[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type Q8_0 | T+  45\n",
            "[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type Q8_0 | T+  45\n",
            "[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  45\n",
            "[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type Q8_0 | T+  45\n",
            "[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type Q8_0 | T+  45\n",
            "[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type Q8_0 | T+  45\n",
            "[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type Q8_0 | T+  45\n",
            "[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  45\n",
            "[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  47\n",
            "[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  48\n",
            "[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  48\n",
            "[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  48\n",
            "[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  48\n",
            "[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  48\n",
            "[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  48\n",
            "[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  48\n",
            "[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  48\n",
            "[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  49\n",
            "[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  50\n",
            "[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  50\n",
            "[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  50\n",
            "[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  50\n",
            "[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  50\n",
            "[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  50\n",
            "[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  50\n",
            "[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  50\n",
            "[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  51\n",
            "[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  52\n",
            "[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  52\n",
            "[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  53\n",
            "[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  53\n",
            "[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  53\n",
            "[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  53\n",
            "[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  53\n",
            "[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  53\n",
            "[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  54\n",
            "[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  54\n",
            "[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  55\n",
            "[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  55\n",
            "[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  55\n",
            "[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  55\n",
            "[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  55\n",
            "[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  55\n",
            "[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  55\n",
            "[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  56\n",
            "[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  56\n",
            "[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  57\n",
            "[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  57\n",
            "[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  57\n",
            "[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  57\n",
            "[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  57\n",
            "[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  57\n",
            "[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  57\n",
            "[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  59\n",
            "[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  60\n",
            "[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  60\n",
            "[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  60\n",
            "[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  60\n",
            "[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  60\n",
            "[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  60\n",
            "[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  60\n",
            "[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  60\n",
            "[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type Q8_0 | T+  62\n",
            "[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type Q8_0 | T+  62\n",
            "[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type Q8_0 | T+  62\n",
            "[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  62\n",
            "[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type Q8_0 | T+  62\n",
            "[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type Q8_0 | T+  62\n",
            "[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type Q8_0 | T+  62\n",
            "[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type Q8_0 | T+  62\n",
            "[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  62\n",
            "Wrote Tinyllama-1b-v1.0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lash Tinyllama-1b-v1.0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62wHFLjZz-7w",
        "outputId": "06051e93-4fa0-4d7c-fe29-e02b27a35391"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1G -rw-r--r-- 1 root root 1.1G Jan 14 16:32 Tinyllama-1b-v1.0.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "2ob7Cgpy1CsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"epsil/Tinyllama-1b-v1.0-gguf\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"Tinyllama-1b-v1.0.gguf\",\n",
        "    path_in_repo=\"Tinyllama-1b-v1.0.gguf\",\n",
        "    repo_id=model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "SEgvK1ia0RLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of Model"
      ],
      "metadata": {
        "id": "_UL9EsB72nh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ctransformers[cuda]"
      ],
      "metadata": {
        "id": "yh3pZJim2Mts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"epsil/Tinyllama-1b-v1.0-gguf\", model_file=\"Tinyllama-1b-v1.0.gguf\")\n",
        "\n",
        "print(llm(\"AI is going to\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "c060246673fb4bf2b1372637f03c8645",
            "61b14c578d5a4d588321a470ae684f87",
            "25b5e5ffbb0b431abe901c011b2eb82c",
            "566e5a707e6741c29c3d55e1e17801e5",
            "e7959e12a98c4942800a0d08ca20a1c4",
            "70a746fdcce14e7c98792f6246b5900e",
            "b381db17a98a4e86802a117942cb874f",
            "34018537c71b4f3b9c3981faadb4dd31",
            "a804f1cd730a46a69591daade8ee7171",
            "1b1b8f34dca14a3ba48953e73c359dc9",
            "96bc14f5b3ff4e4394e2f7d99d59af24",
            "9fc3c232e4eb4b3dae9350b835948017",
            "4f5ea4dc8790427aa4928af3dc0585f0",
            "854c5bcee18a44aebe5336e0953c789b",
            "8dad9520383a435a81eea39b1af68606",
            "81713d997e0b4d30880d9b0da0f91463",
            "5c9a5f894679490793e7eac2f8148e7a",
            "5c231daf767946008ea00161f4aaee4c",
            "b25a00c4dbb84b2599e730dfdea97e50",
            "be214afa682544dfadbd1db0b8364c01",
            "77a4bfbdcb4a412bafdb725ba481a727",
            "f3b5af96bb5741e58f625bb90e6a6e2b",
            "8f9bf24ca34646edbd25378d7ffb83dd",
            "24df9ca3bd2f43d181afc2ebc315bd28",
            "20d4f15c1c004591a9ea326580ea1b56",
            "6f11a8bd6b7447dd93108e0a4d7ee0e1",
            "659cd6b71ffb487abc06cda577582779",
            "dfa61c6e64dc445aa948a3c2d27305e3",
            "676cecb1529d411fa502fe9113f9e122",
            "43f9f7029e694337898841411687ff81",
            "4fc3df945bc449b58970e4a011f5c785",
            "6652ec1352744c6683ecce3b3f6064fa",
            "01407200482e40eb8f4bd09821b240a6"
          ]
        },
        "id": "91FU1zGK0uKV",
        "outputId": "2619de5b-d2d7-458a-b9a4-34737785dcce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 0 files: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c060246673fb4bf2b1372637f03c8645"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fc3c232e4eb4b3dae9350b835948017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tinyllama-1b-v1.0.gguf:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f9bf24ca34646edbd25378d7ffb83dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " make our lives better? I’m excited about the possibilities. Can you help me understand more about how it works and what specific industries it could be used in? I’m a marketer, so I want to know how this technology can benefit my business.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBSCL48I2Qqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}