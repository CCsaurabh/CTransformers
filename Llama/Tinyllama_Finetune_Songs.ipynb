{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6M78AslDpgAq/o2EYTo25"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "KMJuaLpZ2yY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "DUDX9LA12zOx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Dataset"
      ],
      "metadata": {
        "id": "RScXLajuIphZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXunKJ35JQyJ",
        "outputId": "1fe13a84-c190-44ae-c456-dcba9d152382"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: 01-taylor_swift.csv     \n",
            "  inflating: 02-fearless_taylors_version.csv  \n",
            "  inflating: 03-speak_now_deluxe_package.csv  \n",
            "  inflating: 04-red_deluxe_edition.csv  \n",
            "  inflating: 05-1989_deluxe.csv      \n",
            "  inflating: 06-reputation.csv       \n",
            "  inflating: 07-lover.csv            \n",
            "  inflating: 08-folklore_deluxe_version.csv  \n",
            "  inflating: 09-evermore_deluxe_version.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "import glob\n",
        "import pandas as pd\n",
        "path = '.'\n",
        "csv_files = glob.glob(path + \"/*.csv\")\n",
        "df_list = (pd.read_csv(i) for i in csv_files)\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "lyrics = '\\n'.join(df.loc[:,'lyric'])\n",
        "print(lyrics[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEfw1x3EJQ02",
        "outputId": "d746587a-90a6-4763-b74e-b329a2defd17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Knew he was a killer first time that I saw him\n",
            "Wondered how many girls he had loved and left haunted\n",
            "But if he's a ghost, then I can be a phantom\n",
            "Holdin' him for ransom, some\n",
            "Some boys are tryin' too \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all unique characters\n",
        "print(' '.join(sorted(set(lyrics))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS5FckUWJ8Ld",
        "outputId": "c5dc2298-3ef3-490b-e59c-b4bd41021ab6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! \" & ' ( ) , - . 0 1 2 3 4 5 6 7 8 9 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y [ ] a b c d e f g h i j k l m n o p q r s t u v w x y z |   é í ï ó е   ​ – — ‘ ’ ” …  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
        "from trl import SFTTrainer\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "kShFvwD2KbeE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cleaning the file by removing/replacing unnecessary characters and removing sections\n",
        "# that are not lyrics\n",
        "replace_with_space = ['\\u2005', '\\u200b', '\\u205f', '\\xa0', '-']\n",
        "replace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\\'', '’':'\\'', ':':',', 'е':'e'}\n",
        "remove_list = ['\\)', '\\(', '–','\"','”', '\"', '\\[.*\\]', '.*\\|.*', '—']\n",
        "\n",
        "cleaned_lyrics = lyrics\n",
        "\n",
        "for old, new in replace_letters.items():\n",
        "    cleaned_lyrics = cleaned_lyrics.replace(old, new)\n",
        "for string in remove_list:\n",
        "    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)\n",
        "for string in replace_with_space:\n",
        "    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)\n",
        "print(''.join(sorted(set(cleaned_lyrics))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAcYWo68J8PB",
        "outputId": "bc8d3b30-ca20-4cd8-cd49-4a5b5ab26faa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !',.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting aside a portion for training the model and a portion for testing the data to prevent\n",
        "# the model from overfitting to the data it is tested on\n",
        "split_point = int(len(cleaned_lyrics)*0.95)\n",
        "train_data = cleaned_lyrics[:split_point]\n",
        "test_data = cleaned_lyrics[split_point:]\n",
        "train_data_seg = []\n",
        "for i in range(0, len(train_data), 500):\n",
        "        text = train_data[i:min(i+500, len(train_data))]\n",
        "        train_data_seg.append(text)\n",
        "train_data_seg = Dataset.from_dict({'text':train_data_seg})\n",
        "print(len(train_data_seg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tzzBG3wJ8TG",
        "outputId": "efd1dc55-8b7a-49bf-e250-d85f9628d81d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model with double quantization\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "5USY8X51K4-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenizer and defining the pad token\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "Okb1ACJXLAta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating lyrics with the base model. The repetition penalty in the generation config prevents the model from continually repeating the same string.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def generate_lyrics(query, model):\n",
        "    encoding = tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "    generation_config = GenerationConfig(max_new_tokens=250, pad_token_id = tokenizer.eos_token_id,repetition_penalty=1.3, eos_token_id = tokenizer.eos_token_id)\n",
        "    outputs = model.generate(input_ids=encoding.input_ids, generation_config=generation_config)\n",
        "    text_output = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "    print('INPUT\\n', query, '\\n\\nOUTPUT\\n', text_output[len(query):])\n",
        "generate_lyrics(test_data[200:700], model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg7AS4QHLNXY",
        "outputId": "245d0b99-5644-4027-efa0-a7d7525595be"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT\n",
            " urself, talk to the tears\n",
            "Talk to the man who put you here\n",
            "And don't wait for the sky to clear\n",
            "I'll leave my window open\n",
            "'Cause I'm too tired at night to call your name, oh\n",
            "Just know I'm right here hoping\n",
            "That you'll come in with the rain\n",
            "I've watched you so long, screamed your name\n",
            "I don't know what else I can say\n",
            "I'll leave my window open\n",
            "'Cause I'm too tired at night for all these games\n",
            "Just know I'm right here hoping\n",
            "That you'll come in with the rain\n",
            "I could go back to every laugh\n",
            "But I don' \n",
            "\n",
            "OUTPUT\n",
            " t want that anymore\n",
            "So let me just be a little sadder tonight\n",
            "Let it sink into my heart and mind\n",
            "For now, I won't cry out loud\n",
            "Because there are other things worth feeling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_1uBBcsLNaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting arguments for low-rank adaptation\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_alpha = 32 # The weight matrix is scaled by lora_alpha/lora_rank, so I set lora_alpha = lora_rank to remove scaling\n",
        "lora_dropout = 0.05\n",
        "lora_rank = 32\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_rank,\n",
        "    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n",
        "    task_type=\"CAUSAL_LM\")\n",
        "\n",
        "peft_model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "1Ftl2S-XLNcu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting training arguments\n",
        "\n",
        "output_dir = \"epsil/tinyllama_songs\" # Model repo on your hugging face account where you want to save your model\n",
        "per_device_train_batch_size = 3\n",
        "gradient_accumulation_steps = 2\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_strategy=\"steps\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-3\n",
        "max_grad_norm = 0.3 # Sets limit for gradient clipping\n",
        "max_steps = 200     # Number of training steps\n",
        "warmup_ratio = 0.03 # Portion of steps used for learning_rate to warmup from 0\n",
        "lr_scheduler_type = \"cosine\" # I chose cosine to avoid learning plateaus"
      ],
      "metadata": {
        "id": "r7maq9RxLNfu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    push_to_hub=True,\n",
        "    report_to='none'\n",
        ")"
      ],
      "metadata": {
        "id": "gD3AEwMiLNie"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ],
      "metadata": {
        "id": "QqXRkmL3MW9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "nfkGCCyUMiBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_data_seg,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=500,\n",
        "    dataset_text_field='text',\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")\n",
        "peft_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "19JPGbdLLNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "u3RcFFVXMv0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating lyrics with fine-tuned model\n",
        "generate_lyrics(test_data[200:700], model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-3_YPQMv3k",
        "outputId": "64daa8a6-ca63-4f73-9ee3-84c552b34df1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT\n",
            " urself, talk to the tears\n",
            "Talk to the man who put you here\n",
            "And don't wait for the sky to clear\n",
            "I'll leave my window open\n",
            "'Cause I'm too tired at night to call your name, oh\n",
            "Just know I'm right here hoping\n",
            "That you'll come in with the rain\n",
            "I've watched you so long, screamed your name\n",
            "I don't know what else I can say\n",
            "I'll leave my window open\n",
            "'Cause I'm too tired at night for all these games\n",
            "Just know I'm right here hoping\n",
            "That you'll come in with the rain\n",
            "I could go back to every laugh\n",
            "But I don' \n",
            "\n",
            "OUTPUT\n",
            " t think it matters now\n",
            "You left me a note on the kitchen table\n",
            "Said that we were both just kids when this started\n",
            "So please tell me why?\n",
            "Please tell me how much of this is real\n",
            "How many times have I said sorry already\n",
            "Why do people always fall into love like they did us?\n",
            "It was such an easy thing to forget\n",
            "We had everything and nothing seemed new or fresh\n",
            "Now there are days where I wake up feeling empty\n",
            "Feelings that no one ever felt before\n",
            "No matter how hard I try\n",
            "To find something worth fighting for\n",
            "There ain't nothin' wrong with ya\n",
            "If only you knew\n",
            "Oh, if only you knew\n",
            "This song will be played forevermore\n",
            "In our house until the day we die\n",
            "Ooh ooh ooh ooh ooh\n",
            "Hey, hey, hey!\n",
            "Don't let nobody take away your light 'til mine goes out\n",
            "Heartbreaks make diamonds shine brighter than any other color\n",
            "Give them time, give them space\n",
            "Let em breathe, but never burn\n",
            "They need someone to hold their hand through the fire\n",
            "Holding hands through the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6qBXlY_gM6tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BY8x0TwZ9Y9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Kz8r-Dx9ZNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References :\n",
        "\n",
        "https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb\n",
        "\n",
        "https://huggingface.co/TinyLlama\n",
        "\n",
        "https://www.kaggle.com/code/tommyadams/fine-tuning-tinyllama"
      ],
      "metadata": {
        "id": "vm2kmj5h3cQr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKFO7Y833fsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}