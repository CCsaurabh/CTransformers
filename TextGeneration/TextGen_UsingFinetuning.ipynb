{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGen_UsingFinetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7/hSoRpOV9DkgDqKTDNCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CCsaurabh/CTransformers/blob/main/TextGeneration/TextGen_UsingFinetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxht5fIbCbji",
        "outputId": "655e238a-aed4-4fda-c863-ce86c040562b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 94638, done.\u001b[K\n",
            "remote: Total 94638 (delta 0), reused 0 (delta 0), pack-reused 94638\u001b[K\n",
            "Receiving objects: 100% (94638/94638), 79.84 MiB | 23.61 MiB/s, done.\n",
            "Resolving deltas: 100% (68717/68717), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "# Use language modeling version as of April 21st.\n",
        "!git checkout b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BovpTYPiDVG8",
        "outputId": "ad42ab30-d39d-42a3-c4c9-636f59fbc0cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: checking out 'b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at b1ff0b2ae Fix bug in examples: double wrap into DataParallel during eval\n",
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Collecting tokenizers==0.7.0rc7\n",
            "  Downloading tokenizers-0.7.0rc7-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.26-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.26\n",
            "  Downloading botocore-1.23.26-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 34.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 46.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.26->boto3->transformers==2.8.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.26->boto3->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-py3-none-any.whl size=570775 sha256=4b52d262820674ecc4d1fb51e2fd6f5e1de0294f0072a3d73492ee542de18ef3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i0knvbl_/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.26 botocore-1.23.26 jmespath-0.10.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.7.0rc7 transformers-2.8.0 urllib3-1.25.11\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 3)) (1.0.1)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 8)) (4.0.1)\n",
            "Collecting pytorch-lightning==0.7.3\n",
            "  Downloading pytorch_lightning-0.7.3-py3-none-any.whl (203 kB)\n",
            "\u001b[K     |████████████████████████████████| 203 kB 42.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (1.19.5)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (4.62.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard->-r ./examples/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (2019.12.20)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.1.6)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (5.4.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (21.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.53.0)\n",
            "Building wheels for collected packages: future, seqeval\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=ca8ac6e6e84511c3dfe0a32174b4a5f998867a4d98b29fbabe7a834028248c74\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=dae2637bbcb9d2a034f9dd6553d06e74e70506ce20722ce2809cdbf3d7ffeb44\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built future seqeval\n",
            "Installing collected packages: portalocker, future, colorama, tensorboardX, seqeval, sacrebleu, rouge-score, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed colorama-0.4.4 future-0.18.2 portalocker-2.3.2 pytorch-lightning-0.7.3 rouge-score-0.0.4 sacrebleu-2.0.0 seqeval-1.2.2 tensorboardX-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict-to-object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na-ctA6JDoDV",
        "outputId": "24b7086d-fe9b-4d3f-e572-54be3e9b7a9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dict-to-object in /usr/local/lib/python3.7/dist-packages (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "import dicttoobject\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead"
      ],
      "metadata": {
        "id": "UlI5pXsCDwbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joJDVHWiE8Su",
        "outputId": "708faf5f-b53a-499d-b9bf-1ddc60ac28f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the train and test set.\n",
        "!wget -nc -O /content/test.txt https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_test.txt\n",
        "!wget -nc -O /content/valid.txt https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_valid.txt\n",
        "!wget -nc -O /content/train.txt https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXJ36cgfHVin",
        "outputId": "4e95ee18-19b9-40d7-c9d9-146fe92904ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-29 12:18:01--  https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_test.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/test.txt’\n",
            "\n",
            "/content/test.txt       [ <=>                ]   1.15M  7.58MB/s    in 0.2s    \n",
            "\n",
            "2021-12-29 12:18:01 (7.58 MB/s) - ‘/content/test.txt’ saved [1210137]\n",
            "\n",
            "--2021-12-29 12:18:02--  https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_valid.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/valid.txt’\n",
            "\n",
            "/content/valid.txt      [ <=>                ]   1.28M  8.40MB/s    in 0.2s    \n",
            "\n",
            "2021-12-29 12:18:02 (8.40 MB/s) - ‘/content/valid.txt’ saved [1341584]\n",
            "\n",
            "--2021-12-29 12:18:02--  https://github.com/interactive-fiction-class/interactive-fiction-class.github.io/blob/master/homeworks/language-model/text_adventures_train.txt\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/train.txt’\n",
            "\n",
            "/content/train.txt      [ <=>                ] 166.10K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-12-29 12:18:03 (2.12 MB/s) - ‘/content/train.txt’ saved [170083]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/presidential_speeches' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=500 \\\n",
        "    --train_data_file=/content/train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=32 \\\n",
        "    --gradient_accumulation_steps=5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_pHlYhCIDP_",
        "outputId": "238a81fd-548b-4f74-9b1f-29dd43a81f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/29/2021 12:20:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "12/29/2021 12:20:48 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp6n8n06tv\n",
            "Downloading: 100% 665/665 [00:00<00:00, 614kB/s]\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:20:49 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:20:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:20:49 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:20:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_l6p487p\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 5.75MB/s]\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 12:20:49 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp66u6plfa\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.69MB/s]\n",
            "12/29/2021 12:20:50 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 12:20:50 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 12:20:50 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 12:20:50 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 12:20:50 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpq0802_ls\n",
            "Downloading: 100% 548M/548M [00:13<00:00, 41.0MB/s]\n",
            "12/29/2021 12:21:04 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "12/29/2021 12:21:04 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "12/29/2021 12:21:04 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "12/29/2021 12:21:08 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "12/29/2021 12:21:19 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=32, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/valid.txt', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=5, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/drive/My Drive/finetuned_models/presidential_speeches', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=500, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train.txt', warmup_steps=0, weight_decay=0.0)\n",
            "12/29/2021 12:21:19 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "12/29/2021 12:21:20 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_32_train.txt\n",
            "12/29/2021 12:21:20 - INFO - __main__ -   ***** Running training *****\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Num examples = 2382\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Num Epochs = 1\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Gradient Accumulation steps = 5\n",
            "12/29/2021 12:21:20 - INFO - __main__ -     Total optimization steps = 238\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/1191 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/modeling_gpt2.py:149: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n",
            "  w = torch.where(mask, w, self.masked_bias)\n",
            "\n",
            "Iteration:   0% 1/1191 [00:01<20:32,  1.04s/it]\u001b[A\n",
            "Iteration:   0% 3/1191 [00:01<06:39,  2.98it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   0% 5/1191 [00:01<04:27,  4.43it/s]\u001b[A\n",
            "Iteration:   1% 7/1191 [00:01<03:18,  5.97it/s]\u001b[A\n",
            "Iteration:   1% 9/1191 [00:01<02:36,  7.56it/s]\u001b[A\n",
            "Iteration:   1% 11/1191 [00:02<02:28,  7.93it/s]\u001b[A\n",
            "Iteration:   1% 13/1191 [00:02<02:07,  9.22it/s]\u001b[A\n",
            "Iteration:   1% 15/1191 [00:02<02:03,  9.52it/s]\u001b[A\n",
            "Iteration:   1% 17/1191 [00:02<01:57,  9.96it/s]\u001b[A\n",
            "Iteration:   2% 19/1191 [00:02<01:47, 10.87it/s]\u001b[A\n",
            "Iteration:   2% 21/1191 [00:02<01:55, 10.14it/s]\u001b[A\n",
            "Iteration:   2% 23/1191 [00:03<01:45, 11.05it/s]\u001b[A\n",
            "Iteration:   2% 25/1191 [00:03<01:49, 10.69it/s]\u001b[A\n",
            "Iteration:   2% 27/1191 [00:03<01:48, 10.74it/s]\u001b[A\n",
            "Iteration:   2% 29/1191 [00:03<01:40, 11.51it/s]\u001b[A\n",
            "Iteration:   3% 31/1191 [00:03<01:49, 10.59it/s]\u001b[A\n",
            "Iteration:   3% 33/1191 [00:03<01:41, 11.37it/s]\u001b[A\n",
            "Iteration:   3% 35/1191 [00:04<01:44, 11.06it/s]\u001b[A\n",
            "Iteration:   3% 37/1191 [00:04<01:45, 10.96it/s]\u001b[A\n",
            "Iteration:   3% 39/1191 [00:04<01:39, 11.62it/s]\u001b[A\n",
            "Iteration:   3% 41/1191 [00:04<01:49, 10.50it/s]\u001b[A\n",
            "Iteration:   4% 43/1191 [00:04<01:41, 11.33it/s]\u001b[A\n",
            "Iteration:   4% 45/1191 [00:05<01:43, 11.03it/s]\u001b[A\n",
            "Iteration:   4% 47/1191 [00:05<01:44, 10.90it/s]\u001b[A\n",
            "Iteration:   4% 49/1191 [00:05<01:37, 11.66it/s]\u001b[A\n",
            "Iteration:   4% 51/1191 [00:05<01:46, 10.68it/s]\u001b[A\n",
            "Iteration:   4% 53/1191 [00:05<01:39, 11.46it/s]\u001b[A\n",
            "Iteration:   5% 55/1191 [00:05<01:42, 11.09it/s]\u001b[A\n",
            "Iteration:   5% 57/1191 [00:06<01:42, 11.08it/s]\u001b[A\n",
            "Iteration:   5% 59/1191 [00:06<01:36, 11.77it/s]\u001b[A\n",
            "Iteration:   5% 61/1191 [00:06<01:45, 10.74it/s]\u001b[A\n",
            "Iteration:   5% 63/1191 [00:06<01:39, 11.31it/s]\u001b[A\n",
            "Iteration:   5% 65/1191 [00:06<01:41, 11.08it/s]\u001b[A\n",
            "Iteration:   6% 67/1191 [00:07<01:42, 10.94it/s]\u001b[A\n",
            "Iteration:   6% 69/1191 [00:07<01:36, 11.65it/s]\u001b[A\n",
            "Iteration:   6% 71/1191 [00:07<01:45, 10.62it/s]\u001b[A\n",
            "Iteration:   6% 73/1191 [00:07<01:38, 11.37it/s]\u001b[A\n",
            "Iteration:   6% 75/1191 [00:07<01:40, 11.11it/s]\u001b[A\n",
            "Iteration:   6% 77/1191 [00:07<01:40, 11.07it/s]\u001b[A\n",
            "Iteration:   7% 79/1191 [00:08<01:36, 11.58it/s]\u001b[A\n",
            "Iteration:   7% 81/1191 [00:08<01:44, 10.60it/s]\u001b[A\n",
            "Iteration:   7% 83/1191 [00:08<01:37, 11.41it/s]\u001b[A\n",
            "Iteration:   7% 85/1191 [00:08<01:39, 11.16it/s]\u001b[A\n",
            "Iteration:   7% 87/1191 [00:08<01:39, 11.14it/s]\u001b[A\n",
            "Iteration:   7% 89/1191 [00:08<01:33, 11.78it/s]\u001b[A\n",
            "Iteration:   8% 91/1191 [00:09<01:42, 10.74it/s]\u001b[A\n",
            "Iteration:   8% 93/1191 [00:09<01:35, 11.53it/s]\u001b[A\n",
            "Iteration:   8% 95/1191 [00:09<01:38, 11.18it/s]\u001b[A\n",
            "Iteration:   8% 97/1191 [00:09<01:38, 11.12it/s]\u001b[A\n",
            "Iteration:   8% 99/1191 [00:09<01:33, 11.74it/s]\u001b[A\n",
            "Iteration:   8% 101/1191 [00:10<01:44, 10.46it/s]\u001b[A\n",
            "Iteration:   9% 103/1191 [00:10<01:36, 11.23it/s]\u001b[A\n",
            "Iteration:   9% 105/1191 [00:10<01:38, 10.99it/s]\u001b[A\n",
            "Iteration:   9% 107/1191 [00:10<01:38, 10.98it/s]\u001b[A\n",
            "Iteration:   9% 109/1191 [00:10<01:32, 11.70it/s]\u001b[A\n",
            "Iteration:   9% 111/1191 [00:10<01:41, 10.63it/s]\u001b[A\n",
            "Iteration:   9% 113/1191 [00:11<01:34, 11.46it/s]\u001b[A\n",
            "Iteration:  10% 115/1191 [00:11<01:36, 11.20it/s]\u001b[A\n",
            "Iteration:  10% 117/1191 [00:11<01:36, 11.18it/s]\u001b[A\n",
            "Iteration:  10% 119/1191 [00:11<01:30, 11.87it/s]\u001b[A\n",
            "Iteration:  10% 121/1191 [00:11<01:40, 10.63it/s]\u001b[A\n",
            "Iteration:  10% 123/1191 [00:12<01:33, 11.40it/s]\u001b[A\n",
            "Iteration:  10% 125/1191 [00:12<01:35, 11.16it/s]\u001b[A\n",
            "Iteration:  11% 127/1191 [00:12<01:36, 11.06it/s]\u001b[A\n",
            "Iteration:  11% 129/1191 [00:12<01:31, 11.61it/s]\u001b[A\n",
            "Iteration:  11% 131/1191 [00:12<01:40, 10.59it/s]\u001b[A\n",
            "Iteration:  11% 133/1191 [00:12<01:33, 11.35it/s]\u001b[A\n",
            "Iteration:  11% 135/1191 [00:13<01:36, 10.93it/s]\u001b[A\n",
            "Iteration:  12% 137/1191 [00:13<01:37, 10.85it/s]\u001b[A\n",
            "Iteration:  12% 139/1191 [00:13<01:30, 11.57it/s]\u001b[A\n",
            "Iteration:  12% 141/1191 [00:13<01:39, 10.55it/s]\u001b[A\n",
            "Iteration:  12% 143/1191 [00:13<01:33, 11.23it/s]\u001b[A\n",
            "Iteration:  12% 145/1191 [00:14<01:35, 10.94it/s]\u001b[A\n",
            "Iteration:  12% 147/1191 [00:14<01:35, 10.90it/s]\u001b[A\n",
            "Iteration:  13% 149/1191 [00:14<01:29, 11.62it/s]\u001b[A\n",
            "Iteration:  13% 151/1191 [00:14<01:37, 10.63it/s]\u001b[A\n",
            "Iteration:  13% 153/1191 [00:14<01:32, 11.28it/s]\u001b[A\n",
            "Iteration:  13% 155/1191 [00:14<01:33, 11.06it/s]\u001b[A\n",
            "Iteration:  13% 157/1191 [00:15<01:35, 10.80it/s]\u001b[A\n",
            "Iteration:  13% 159/1191 [00:15<01:29, 11.51it/s]\u001b[A\n",
            "Iteration:  14% 161/1191 [00:15<01:37, 10.55it/s]\u001b[A\n",
            "Iteration:  14% 163/1191 [00:15<01:31, 11.26it/s]\u001b[A\n",
            "Iteration:  14% 165/1191 [00:15<01:33, 10.92it/s]\u001b[A\n",
            "Iteration:  14% 167/1191 [00:16<01:33, 10.92it/s]\u001b[A\n",
            "Iteration:  14% 169/1191 [00:16<01:28, 11.55it/s]\u001b[A\n",
            "Iteration:  14% 171/1191 [00:16<01:36, 10.60it/s]\u001b[A\n",
            "Iteration:  15% 173/1191 [00:16<01:29, 11.36it/s]\u001b[A\n",
            "Iteration:  15% 175/1191 [00:16<01:31, 11.13it/s]\u001b[A\n",
            "Iteration:  15% 177/1191 [00:16<01:31, 11.10it/s]\u001b[A\n",
            "Iteration:  15% 179/1191 [00:17<01:26, 11.66it/s]\u001b[A\n",
            "Iteration:  15% 181/1191 [00:17<01:34, 10.66it/s]\u001b[A\n",
            "Iteration:  15% 183/1191 [00:17<01:28, 11.38it/s]\u001b[A\n",
            "Iteration:  16% 185/1191 [00:17<01:31, 11.02it/s]\u001b[A\n",
            "Iteration:  16% 187/1191 [00:17<01:30, 11.05it/s]\u001b[A\n",
            "Iteration:  16% 189/1191 [00:17<01:26, 11.60it/s]\u001b[A\n",
            "Iteration:  16% 191/1191 [00:18<01:35, 10.49it/s]\u001b[A\n",
            "Iteration:  16% 193/1191 [00:18<01:28, 11.27it/s]\u001b[A\n",
            "Iteration:  16% 195/1191 [00:18<01:31, 10.93it/s]\u001b[A\n",
            "Iteration:  17% 197/1191 [00:18<01:30, 11.01it/s]\u001b[A\n",
            "Iteration:  17% 199/1191 [00:18<01:25, 11.60it/s]\u001b[A\n",
            "Iteration:  17% 201/1191 [00:19<01:33, 10.61it/s]\u001b[A\n",
            "Iteration:  17% 203/1191 [00:19<01:27, 11.27it/s]\u001b[A\n",
            "Iteration:  17% 205/1191 [00:19<01:28, 11.11it/s]\u001b[A\n",
            "Iteration:  17% 207/1191 [00:19<01:29, 11.03it/s]\u001b[A\n",
            "Iteration:  18% 209/1191 [00:19<01:23, 11.74it/s]\u001b[A\n",
            "Iteration:  18% 211/1191 [00:19<01:31, 10.66it/s]\u001b[A\n",
            "Iteration:  18% 213/1191 [00:20<01:25, 11.47it/s]\u001b[A\n",
            "Iteration:  18% 215/1191 [00:20<01:27, 11.11it/s]\u001b[A\n",
            "Iteration:  18% 217/1191 [00:20<01:27, 11.07it/s]\u001b[A\n",
            "Iteration:  18% 219/1191 [00:20<01:22, 11.73it/s]\u001b[A\n",
            "Iteration:  19% 221/1191 [00:20<01:30, 10.69it/s]\u001b[A\n",
            "Iteration:  19% 223/1191 [00:21<01:26, 11.22it/s]\u001b[A\n",
            "Iteration:  19% 225/1191 [00:21<01:29, 10.74it/s]\u001b[A\n",
            "Iteration:  19% 227/1191 [00:21<01:28, 10.84it/s]\u001b[A\n",
            "Iteration:  19% 229/1191 [00:21<01:23, 11.56it/s]\u001b[A\n",
            "Iteration:  19% 231/1191 [00:21<01:31, 10.53it/s]\u001b[A\n",
            "Iteration:  20% 233/1191 [00:21<01:24, 11.29it/s]\u001b[A\n",
            "Iteration:  20% 235/1191 [00:22<01:27, 10.98it/s]\u001b[A\n",
            "Iteration:  20% 237/1191 [00:22<01:27, 10.96it/s]\u001b[A\n",
            "Iteration:  20% 239/1191 [00:22<01:21, 11.68it/s]\u001b[A\n",
            "Iteration:  20% 241/1191 [00:22<01:29, 10.62it/s]\u001b[A\n",
            "Iteration:  20% 243/1191 [00:22<01:23, 11.41it/s]\u001b[A\n",
            "Iteration:  21% 245/1191 [00:23<01:26, 10.94it/s]\u001b[A\n",
            "Iteration:  21% 247/1191 [00:23<01:26, 10.89it/s]\u001b[A\n",
            "Iteration:  21% 249/1191 [00:23<01:23, 11.33it/s]\u001b[A\n",
            "Iteration:  21% 251/1191 [00:23<01:30, 10.44it/s]\u001b[A\n",
            "Iteration:  21% 253/1191 [00:23<01:23, 11.23it/s]\u001b[A\n",
            "Iteration:  21% 255/1191 [00:23<01:25, 10.93it/s]\u001b[A\n",
            "Iteration:  22% 257/1191 [00:24<01:25, 10.96it/s]\u001b[A\n",
            "Iteration:  22% 259/1191 [00:24<01:19, 11.66it/s]\u001b[A\n",
            "Iteration:  22% 261/1191 [00:24<01:28, 10.56it/s]\u001b[A\n",
            "Iteration:  22% 263/1191 [00:24<01:22, 11.27it/s]\u001b[A\n",
            "Iteration:  22% 265/1191 [00:24<01:24, 10.97it/s]\u001b[A\n",
            "Iteration:  22% 267/1191 [00:25<01:23, 11.01it/s]\u001b[A\n",
            "Iteration:  23% 269/1191 [00:25<01:19, 11.65it/s]\u001b[A\n",
            "Iteration:  23% 271/1191 [00:25<01:26, 10.58it/s]\u001b[A\n",
            "Iteration:  23% 273/1191 [00:25<01:21, 11.33it/s]\u001b[A\n",
            "Iteration:  23% 275/1191 [00:25<01:23, 10.98it/s]\u001b[A\n",
            "Iteration:  23% 277/1191 [00:25<01:24, 10.87it/s]\u001b[A\n",
            "Iteration:  23% 279/1191 [00:26<01:18, 11.56it/s]\u001b[A\n",
            "Iteration:  24% 281/1191 [00:26<01:25, 10.60it/s]\u001b[A\n",
            "Iteration:  24% 283/1191 [00:26<01:19, 11.37it/s]\u001b[A\n",
            "Iteration:  24% 285/1191 [00:26<01:22, 11.00it/s]\u001b[A\n",
            "Iteration:  24% 287/1191 [00:26<01:22, 10.97it/s]\u001b[A\n",
            "Iteration:  24% 289/1191 [00:26<01:17, 11.71it/s]\u001b[A\n",
            "Iteration:  24% 291/1191 [00:27<01:24, 10.71it/s]\u001b[A\n",
            "Iteration:  25% 293/1191 [00:27<01:18, 11.41it/s]\u001b[A\n",
            "Iteration:  25% 295/1191 [00:27<01:20, 11.15it/s]\u001b[A\n",
            "Iteration:  25% 297/1191 [00:27<01:21, 10.99it/s]\u001b[A\n",
            "Iteration:  25% 299/1191 [00:27<01:16, 11.70it/s]\u001b[A\n",
            "Iteration:  25% 301/1191 [00:28<01:24, 10.48it/s]\u001b[A\n",
            "Iteration:  25% 303/1191 [00:28<01:18, 11.27it/s]\u001b[A\n",
            "Iteration:  26% 305/1191 [00:28<01:21, 10.88it/s]\u001b[A\n",
            "Iteration:  26% 307/1191 [00:28<01:20, 10.94it/s]\u001b[A\n",
            "Iteration:  26% 309/1191 [00:28<01:15, 11.68it/s]\u001b[A\n",
            "Iteration:  26% 311/1191 [00:29<01:22, 10.66it/s]\u001b[A\n",
            "Iteration:  26% 313/1191 [00:29<01:16, 11.41it/s]\u001b[A\n",
            "Iteration:  26% 315/1191 [00:29<01:18, 11.12it/s]\u001b[A\n",
            "Iteration:  27% 317/1191 [00:29<01:19, 10.98it/s]\u001b[A\n",
            "Iteration:  27% 319/1191 [00:29<01:14, 11.70it/s]\u001b[A\n",
            "Iteration:  27% 321/1191 [00:29<01:21, 10.70it/s]\u001b[A\n",
            "Iteration:  27% 323/1191 [00:30<01:16, 11.39it/s]\u001b[A\n",
            "Iteration:  27% 325/1191 [00:30<01:17, 11.15it/s]\u001b[A\n",
            "Iteration:  27% 327/1191 [00:30<01:18, 11.07it/s]\u001b[A\n",
            "Iteration:  28% 329/1191 [00:30<01:13, 11.78it/s]\u001b[A\n",
            "Iteration:  28% 331/1191 [00:30<01:20, 10.63it/s]\u001b[A\n",
            "Iteration:  28% 333/1191 [00:30<01:15, 11.40it/s]\u001b[A\n",
            "Iteration:  28% 335/1191 [00:31<01:16, 11.17it/s]\u001b[A\n",
            "Iteration:  28% 337/1191 [00:31<01:17, 11.05it/s]\u001b[A\n",
            "Iteration:  28% 339/1191 [00:31<01:12, 11.77it/s]\u001b[A\n",
            "Iteration:  29% 341/1191 [00:31<01:19, 10.70it/s]\u001b[A\n",
            "Iteration:  29% 343/1191 [00:31<01:14, 11.45it/s]\u001b[A\n",
            "Iteration:  29% 345/1191 [00:32<01:16, 11.02it/s]\u001b[A\n",
            "Iteration:  29% 347/1191 [00:32<01:16, 11.02it/s]\u001b[A\n",
            "Iteration:  29% 349/1191 [00:32<01:12, 11.66it/s]\u001b[A\n",
            "Iteration:  29% 351/1191 [00:32<01:19, 10.55it/s]\u001b[A\n",
            "Iteration:  30% 353/1191 [00:32<01:14, 11.22it/s]\u001b[A\n",
            "Iteration:  30% 355/1191 [00:32<01:16, 10.95it/s]\u001b[A\n",
            "Iteration:  30% 357/1191 [00:33<01:16, 10.92it/s]\u001b[A\n",
            "Iteration:  30% 359/1191 [00:33<01:11, 11.58it/s]\u001b[A\n",
            "Iteration:  30% 361/1191 [00:33<01:18, 10.63it/s]\u001b[A\n",
            "Iteration:  30% 363/1191 [00:33<01:13, 11.33it/s]\u001b[A\n",
            "Iteration:  31% 365/1191 [00:33<01:15, 10.92it/s]\u001b[A\n",
            "Iteration:  31% 367/1191 [00:34<01:15, 10.98it/s]\u001b[A\n",
            "Iteration:  31% 369/1191 [00:34<01:11, 11.55it/s]\u001b[A\n",
            "Iteration:  31% 371/1191 [00:34<01:17, 10.63it/s]\u001b[A\n",
            "Iteration:  31% 373/1191 [00:34<01:12, 11.28it/s]\u001b[A\n",
            "Iteration:  31% 375/1191 [00:34<01:14, 10.91it/s]\u001b[A\n",
            "Iteration:  32% 377/1191 [00:34<01:14, 10.91it/s]\u001b[A\n",
            "Iteration:  32% 379/1191 [00:35<01:10, 11.57it/s]\u001b[A\n",
            "Iteration:  32% 381/1191 [00:35<01:16, 10.60it/s]\u001b[A\n",
            "Iteration:  32% 383/1191 [00:35<01:10, 11.39it/s]\u001b[A\n",
            "Iteration:  32% 385/1191 [00:35<01:13, 10.96it/s]\u001b[A\n",
            "Iteration:  32% 387/1191 [00:35<01:13, 10.99it/s]\u001b[A\n",
            "Iteration:  33% 389/1191 [00:35<01:09, 11.61it/s]\u001b[A\n",
            "Iteration:  33% 391/1191 [00:36<01:15, 10.65it/s]\u001b[A\n",
            "Iteration:  33% 393/1191 [00:36<01:10, 11.33it/s]\u001b[A\n",
            "Iteration:  33% 395/1191 [00:36<01:11, 11.09it/s]\u001b[A\n",
            "Iteration:  33% 397/1191 [00:36<01:11, 11.07it/s]\u001b[A\n",
            "Iteration:  34% 399/1191 [00:36<01:07, 11.75it/s]\u001b[A\n",
            "Iteration:  34% 401/1191 [00:37<01:13, 10.68it/s]\u001b[A\n",
            "Iteration:  34% 403/1191 [00:37<01:08, 11.46it/s]\u001b[A\n",
            "Iteration:  34% 405/1191 [00:37<01:10, 11.17it/s]\u001b[A\n",
            "Iteration:  34% 407/1191 [00:37<01:10, 11.17it/s]\u001b[A\n",
            "Iteration:  34% 409/1191 [00:37<01:05, 11.85it/s]\u001b[A\n",
            "Iteration:  35% 411/1191 [00:37<01:12, 10.75it/s]\u001b[A\n",
            "Iteration:  35% 413/1191 [00:38<01:08, 11.42it/s]\u001b[A\n",
            "Iteration:  35% 415/1191 [00:38<01:09, 11.19it/s]\u001b[A\n",
            "Iteration:  35% 417/1191 [00:38<01:09, 11.17it/s]\u001b[A\n",
            "Iteration:  35% 419/1191 [00:38<01:05, 11.87it/s]\u001b[A\n",
            "Iteration:  35% 421/1191 [00:38<01:11, 10.76it/s]\u001b[A\n",
            "Iteration:  36% 423/1191 [00:39<01:07, 11.45it/s]\u001b[A\n",
            "Iteration:  36% 425/1191 [00:39<01:08, 11.13it/s]\u001b[A\n",
            "Iteration:  36% 427/1191 [00:39<01:08, 11.12it/s]\u001b[A\n",
            "Iteration:  36% 429/1191 [00:39<01:04, 11.80it/s]\u001b[A\n",
            "Iteration:  36% 431/1191 [00:39<01:10, 10.72it/s]\u001b[A\n",
            "Iteration:  36% 433/1191 [00:39<01:06, 11.46it/s]\u001b[A\n",
            "Iteration:  37% 435/1191 [00:40<01:07, 11.16it/s]\u001b[A\n",
            "Iteration:  37% 437/1191 [00:40<01:08, 11.03it/s]\u001b[A\n",
            "Iteration:  37% 439/1191 [00:40<01:04, 11.70it/s]\u001b[A\n",
            "Iteration:  37% 441/1191 [00:40<01:10, 10.64it/s]\u001b[A\n",
            "Iteration:  37% 443/1191 [00:40<01:07, 11.14it/s]\u001b[A\n",
            "Iteration:  37% 445/1191 [00:41<01:08, 10.93it/s]\u001b[A\n",
            "Iteration:  38% 447/1191 [00:41<01:08, 10.93it/s]\u001b[A\n",
            "Iteration:  38% 449/1191 [00:41<01:04, 11.55it/s]\u001b[A\n",
            "Iteration:  38% 451/1191 [00:41<01:10, 10.57it/s]\u001b[A\n",
            "Iteration:  38% 453/1191 [00:41<01:04, 11.37it/s]\u001b[A\n",
            "Iteration:  38% 455/1191 [00:41<01:06, 11.02it/s]\u001b[A\n",
            "Iteration:  38% 457/1191 [00:42<01:06, 11.04it/s]\u001b[A\n",
            "Iteration:  39% 459/1191 [00:42<01:03, 11.60it/s]\u001b[A\n",
            "Iteration:  39% 461/1191 [00:42<01:09, 10.55it/s]\u001b[A\n",
            "Iteration:  39% 463/1191 [00:42<01:04, 11.37it/s]\u001b[A\n",
            "Iteration:  39% 465/1191 [00:42<01:05, 11.08it/s]\u001b[A\n",
            "Iteration:  39% 467/1191 [00:42<01:05, 11.11it/s]\u001b[A\n",
            "Iteration:  39% 469/1191 [00:43<01:01, 11.68it/s]\u001b[A\n",
            "Iteration:  40% 471/1191 [00:43<01:07, 10.64it/s]\u001b[A\n",
            "Iteration:  40% 473/1191 [00:43<01:03, 11.26it/s]\u001b[A\n",
            "Iteration:  40% 475/1191 [00:43<01:05, 10.86it/s]\u001b[A\n",
            "Iteration:  40% 477/1191 [00:43<01:05, 10.88it/s]\u001b[A\n",
            "Iteration:  40% 479/1191 [00:44<01:02, 11.46it/s]\u001b[A\n",
            "Iteration:  40% 481/1191 [00:44<01:07, 10.51it/s]\u001b[A\n",
            "Iteration:  41% 483/1191 [00:44<01:03, 11.18it/s]\u001b[A\n",
            "Iteration:  41% 485/1191 [00:44<01:05, 10.78it/s]\u001b[A\n",
            "Iteration:  41% 487/1191 [00:44<01:06, 10.61it/s]\u001b[A\n",
            "Iteration:  41% 489/1191 [00:44<01:01, 11.36it/s]\u001b[A\n",
            "Iteration:  41% 491/1191 [00:45<01:06, 10.46it/s]\u001b[A\n",
            "Iteration:  41% 493/1191 [00:45<01:02, 11.21it/s]\u001b[A\n",
            "Iteration:  42% 495/1191 [00:45<01:04, 10.87it/s]\u001b[A\n",
            "Iteration:  42% 497/1191 [00:45<01:03, 10.92it/s]\u001b[A\n",
            "Iteration:  42% 499/1191 [00:45<00:59, 11.56it/s]\u001b[A\n",
            "Iteration:  42% 501/1191 [00:46<01:05, 10.58it/s]\u001b[A\n",
            "Iteration:  42% 503/1191 [00:46<01:01, 11.18it/s]\u001b[A\n",
            "Iteration:  42% 505/1191 [00:46<01:02, 10.97it/s]\u001b[A\n",
            "Iteration:  43% 507/1191 [00:46<01:02, 11.01it/s]\u001b[A\n",
            "Iteration:  43% 509/1191 [00:46<00:58, 11.64it/s]\u001b[A\n",
            "Iteration:  43% 511/1191 [00:47<01:04, 10.46it/s]\u001b[A\n",
            "Iteration:  43% 513/1191 [00:47<01:00, 11.23it/s]\u001b[A\n",
            "Iteration:  43% 515/1191 [00:47<01:01, 10.93it/s]\u001b[A\n",
            "Iteration:  43% 517/1191 [00:47<01:01, 10.97it/s]\u001b[A\n",
            "Iteration:  44% 519/1191 [00:47<00:58, 11.58it/s]\u001b[A\n",
            "Iteration:  44% 521/1191 [00:47<01:03, 10.51it/s]\u001b[A\n",
            "Iteration:  44% 523/1191 [00:48<00:59, 11.29it/s]\u001b[A\n",
            "Iteration:  44% 525/1191 [00:48<01:01, 10.87it/s]\u001b[A\n",
            "Iteration:  44% 527/1191 [00:48<01:00, 10.92it/s]\u001b[A\n",
            "Iteration:  44% 529/1191 [00:48<00:57, 11.42it/s]\u001b[A\n",
            "Iteration:  45% 531/1191 [00:48<01:02, 10.54it/s]\u001b[A\n",
            "Iteration:  45% 533/1191 [00:48<00:59, 11.14it/s]\u001b[A\n",
            "Iteration:  45% 535/1191 [00:49<01:00, 10.85it/s]\u001b[A\n",
            "Iteration:  45% 537/1191 [00:49<01:00, 10.90it/s]\u001b[A\n",
            "Iteration:  45% 539/1191 [00:49<00:56, 11.59it/s]\u001b[A\n",
            "Iteration:  45% 541/1191 [00:49<01:01, 10.64it/s]\u001b[A\n",
            "Iteration:  46% 543/1191 [00:49<00:56, 11.42it/s]\u001b[A\n",
            "Iteration:  46% 545/1191 [00:50<00:58, 10.98it/s]\u001b[A\n",
            "Iteration:  46% 547/1191 [00:50<00:58, 10.99it/s]\u001b[A\n",
            "Iteration:  46% 549/1191 [00:50<00:55, 11.55it/s]\u001b[A\n",
            "Iteration:  46% 551/1191 [00:50<01:00, 10.61it/s]\u001b[A\n",
            "Iteration:  46% 553/1191 [00:50<00:55, 11.40it/s]\u001b[A\n",
            "Iteration:  47% 555/1191 [00:50<00:56, 11.17it/s]\u001b[A\n",
            "Iteration:  47% 557/1191 [00:51<00:56, 11.14it/s]\u001b[A\n",
            "Iteration:  47% 559/1191 [00:51<00:53, 11.85it/s]\u001b[A\n",
            "Iteration:  47% 561/1191 [00:51<00:59, 10.65it/s]\u001b[A\n",
            "Iteration:  47% 563/1191 [00:51<00:55, 11.39it/s]\u001b[A\n",
            "Iteration:  47% 565/1191 [00:51<00:57, 10.85it/s]\u001b[A\n",
            "Iteration:  48% 567/1191 [00:52<00:58, 10.73it/s]\u001b[A\n",
            "Iteration:  48% 569/1191 [00:52<00:55, 11.31it/s]\u001b[A\n",
            "Iteration:  48% 571/1191 [00:52<00:59, 10.43it/s]\u001b[A\n",
            "Iteration:  48% 573/1191 [00:52<00:55, 11.23it/s]\u001b[A\n",
            "Iteration:  48% 575/1191 [00:52<00:56, 10.92it/s]\u001b[A\n",
            "Iteration:  48% 577/1191 [00:52<00:56, 10.96it/s]\u001b[A\n",
            "Iteration:  49% 579/1191 [00:53<00:52, 11.66it/s]\u001b[A\n",
            "Iteration:  49% 581/1191 [00:53<00:57, 10.59it/s]\u001b[A\n",
            "Iteration:  49% 583/1191 [00:53<00:53, 11.39it/s]\u001b[A\n",
            "Iteration:  49% 585/1191 [00:53<00:54, 11.05it/s]\u001b[A\n",
            "Iteration:  49% 587/1191 [00:53<00:55, 10.92it/s]\u001b[A\n",
            "Iteration:  49% 589/1191 [00:54<00:52, 11.41it/s]\u001b[A\n",
            "Iteration:  50% 591/1191 [00:54<00:58, 10.26it/s]\u001b[A\n",
            "Iteration:  50% 593/1191 [00:54<00:53, 11.13it/s]\u001b[A\n",
            "Iteration:  50% 595/1191 [00:54<00:54, 10.89it/s]\u001b[A\n",
            "Iteration:  50% 597/1191 [00:54<00:54, 10.94it/s]\u001b[A\n",
            "Iteration:  50% 599/1191 [00:54<00:51, 11.54it/s]\u001b[A\n",
            "Iteration:  50% 601/1191 [00:55<00:55, 10.60it/s]\u001b[A\n",
            "Iteration:  51% 603/1191 [00:55<00:52, 11.27it/s]\u001b[A\n",
            "Iteration:  51% 605/1191 [00:55<00:53, 11.00it/s]\u001b[A\n",
            "Iteration:  51% 607/1191 [00:55<00:53, 11.00it/s]\u001b[A\n",
            "Iteration:  51% 609/1191 [00:55<00:49, 11.69it/s]\u001b[A\n",
            "Iteration:  51% 611/1191 [00:56<00:55, 10.38it/s]\u001b[A\n",
            "Iteration:  51% 613/1191 [00:56<00:51, 11.17it/s]\u001b[A\n",
            "Iteration:  52% 615/1191 [00:56<00:52, 10.89it/s]\u001b[A\n",
            "Iteration:  52% 617/1191 [00:56<00:52, 10.89it/s]\u001b[A\n",
            "Iteration:  52% 619/1191 [00:56<00:49, 11.52it/s]\u001b[A\n",
            "Iteration:  52% 621/1191 [00:56<00:54, 10.54it/s]\u001b[A\n",
            "Iteration:  52% 623/1191 [00:57<00:50, 11.20it/s]\u001b[A\n",
            "Iteration:  52% 625/1191 [00:57<00:51, 10.96it/s]\u001b[A\n",
            "Iteration:  53% 627/1191 [00:57<00:51, 10.93it/s]\u001b[A\n",
            "Iteration:  53% 629/1191 [00:57<00:48, 11.64it/s]\u001b[A\n",
            "Iteration:  53% 631/1191 [00:57<00:52, 10.58it/s]\u001b[A\n",
            "Iteration:  53% 633/1191 [00:58<00:49, 11.34it/s]\u001b[A\n",
            "Iteration:  53% 635/1191 [00:58<00:51, 10.85it/s]\u001b[A\n",
            "Iteration:  53% 637/1191 [00:58<00:50, 10.88it/s]\u001b[A\n",
            "Iteration:  54% 639/1191 [00:58<00:47, 11.62it/s]\u001b[A\n",
            "Iteration:  54% 641/1191 [00:58<00:52, 10.54it/s]\u001b[A\n",
            "Iteration:  54% 643/1191 [00:58<00:48, 11.28it/s]\u001b[A\n",
            "Iteration:  54% 645/1191 [00:59<00:50, 10.87it/s]\u001b[A\n",
            "Iteration:  54% 647/1191 [00:59<00:49, 10.89it/s]\u001b[A\n",
            "Iteration:  54% 649/1191 [00:59<00:47, 11.31it/s]\u001b[A\n",
            "Iteration:  55% 651/1191 [00:59<00:51, 10.43it/s]\u001b[A\n",
            "Iteration:  55% 653/1191 [00:59<00:47, 11.23it/s]\u001b[A\n",
            "Iteration:  55% 655/1191 [01:00<00:48, 10.98it/s]\u001b[A\n",
            "Iteration:  55% 657/1191 [01:00<00:49, 10.72it/s]\u001b[A\n",
            "Iteration:  55% 659/1191 [01:00<00:46, 11.34it/s]\u001b[A\n",
            "Iteration:  55% 661/1191 [01:00<00:50, 10.47it/s]\u001b[A\n",
            "Iteration:  56% 663/1191 [01:00<00:47, 11.22it/s]\u001b[A\n",
            "Iteration:  56% 665/1191 [01:00<00:48, 10.90it/s]\u001b[A\n",
            "Iteration:  56% 667/1191 [01:01<00:47, 10.98it/s]\u001b[A\n",
            "Iteration:  56% 669/1191 [01:01<00:44, 11.69it/s]\u001b[A\n",
            "Iteration:  56% 671/1191 [01:01<00:48, 10.66it/s]\u001b[A\n",
            "Iteration:  57% 673/1191 [01:01<00:45, 11.44it/s]\u001b[A\n",
            "Iteration:  57% 675/1191 [01:01<00:46, 11.09it/s]\u001b[A\n",
            "Iteration:  57% 677/1191 [01:02<00:46, 10.98it/s]\u001b[A\n",
            "Iteration:  57% 679/1191 [01:02<00:43, 11.71it/s]\u001b[A\n",
            "Iteration:  57% 681/1191 [01:02<00:47, 10.70it/s]\u001b[A\n",
            "Iteration:  57% 683/1191 [01:02<00:44, 11.49it/s]\u001b[A\n",
            "Iteration:  58% 685/1191 [01:02<00:45, 11.03it/s]\u001b[A\n",
            "Iteration:  58% 687/1191 [01:02<00:46, 10.94it/s]\u001b[A\n",
            "Iteration:  58% 689/1191 [01:03<00:43, 11.62it/s]\u001b[A\n",
            "Iteration:  58% 691/1191 [01:03<00:47, 10.48it/s]\u001b[A\n",
            "Iteration:  58% 693/1191 [01:03<00:44, 11.24it/s]\u001b[A\n",
            "Iteration:  58% 695/1191 [01:03<00:45, 10.85it/s]\u001b[A\n",
            "Iteration:  59% 697/1191 [01:03<00:45, 10.74it/s]\u001b[A\n",
            "Iteration:  59% 699/1191 [01:04<00:43, 11.38it/s]\u001b[A\n",
            "Iteration:  59% 701/1191 [01:04<00:46, 10.45it/s]\u001b[A\n",
            "Iteration:  59% 703/1191 [01:04<00:43, 11.25it/s]\u001b[A\n",
            "Iteration:  59% 705/1191 [01:04<00:44, 10.89it/s]\u001b[A\n",
            "Iteration:  59% 707/1191 [01:04<00:44, 10.82it/s]\u001b[A\n",
            "Iteration:  60% 709/1191 [01:04<00:41, 11.56it/s]\u001b[A\n",
            "Iteration:  60% 711/1191 [01:05<00:46, 10.43it/s]\u001b[A\n",
            "Iteration:  60% 713/1191 [01:05<00:43, 11.11it/s]\u001b[A\n",
            "Iteration:  60% 715/1191 [01:05<00:43, 10.91it/s]\u001b[A\n",
            "Iteration:  60% 717/1191 [01:05<00:43, 10.82it/s]\u001b[A\n",
            "Iteration:  60% 719/1191 [01:05<00:41, 11.44it/s]\u001b[A\n",
            "Iteration:  61% 721/1191 [01:06<00:44, 10.52it/s]\u001b[A\n",
            "Iteration:  61% 723/1191 [01:06<00:41, 11.30it/s]\u001b[A\n",
            "Iteration:  61% 725/1191 [01:06<00:42, 10.92it/s]\u001b[A\n",
            "Iteration:  61% 727/1191 [01:06<00:42, 10.96it/s]\u001b[A\n",
            "Iteration:  61% 729/1191 [01:06<00:39, 11.61it/s]\u001b[A\n",
            "Iteration:  61% 731/1191 [01:06<00:43, 10.57it/s]\u001b[A\n",
            "Iteration:  62% 733/1191 [01:07<00:40, 11.34it/s]\u001b[A\n",
            "Iteration:  62% 735/1191 [01:07<00:41, 10.86it/s]\u001b[A\n",
            "Iteration:  62% 737/1191 [01:07<00:41, 10.93it/s]\u001b[A\n",
            "Iteration:  62% 739/1191 [01:07<00:38, 11.62it/s]\u001b[A\n",
            "Iteration:  62% 741/1191 [01:07<00:42, 10.62it/s]\u001b[A\n",
            "Iteration:  62% 743/1191 [01:08<00:39, 11.39it/s]\u001b[A\n",
            "Iteration:  63% 745/1191 [01:08<00:40, 11.07it/s]\u001b[A\n",
            "Iteration:  63% 747/1191 [01:08<00:40, 10.98it/s]\u001b[A\n",
            "Iteration:  63% 749/1191 [01:08<00:37, 11.71it/s]\u001b[A\n",
            "Iteration:  63% 751/1191 [01:08<00:41, 10.68it/s]\u001b[A\n",
            "Iteration:  63% 753/1191 [01:08<00:38, 11.40it/s]\u001b[A\n",
            "Iteration:  63% 755/1191 [01:09<00:39, 11.09it/s]\u001b[A\n",
            "Iteration:  64% 757/1191 [01:09<00:39, 11.02it/s]\u001b[A\n",
            "Iteration:  64% 759/1191 [01:09<00:36, 11.71it/s]\u001b[A\n",
            "Iteration:  64% 761/1191 [01:09<00:40, 10.67it/s]\u001b[A\n",
            "Iteration:  64% 763/1191 [01:09<00:37, 11.45it/s]\u001b[A\n",
            "Iteration:  64% 765/1191 [01:09<00:38, 11.02it/s]\u001b[A\n",
            "Iteration:  64% 767/1191 [01:10<00:38, 10.94it/s]\u001b[A\n",
            "Iteration:  65% 769/1191 [01:10<00:36, 11.61it/s]\u001b[A\n",
            "Iteration:  65% 771/1191 [01:10<00:40, 10.33it/s]\u001b[A\n",
            "Iteration:  65% 773/1191 [01:10<00:37, 11.11it/s]\u001b[A\n",
            "Iteration:  65% 775/1191 [01:10<00:38, 10.90it/s]\u001b[A\n",
            "Iteration:  65% 777/1191 [01:11<00:38, 10.86it/s]\u001b[A\n",
            "Iteration:  65% 779/1191 [01:11<00:35, 11.55it/s]\u001b[A\n",
            "Iteration:  66% 781/1191 [01:11<00:38, 10.57it/s]\u001b[A\n",
            "Iteration:  66% 783/1191 [01:11<00:36, 11.21it/s]\u001b[A\n",
            "Iteration:  66% 785/1191 [01:11<00:36, 10.98it/s]\u001b[A\n",
            "Iteration:  66% 787/1191 [01:11<00:36, 11.00it/s]\u001b[A\n",
            "Iteration:  66% 789/1191 [01:12<00:34, 11.69it/s]\u001b[A\n",
            "Iteration:  66% 791/1191 [01:12<00:37, 10.64it/s]\u001b[A\n",
            "Iteration:  67% 793/1191 [01:12<00:35, 11.21it/s]\u001b[A\n",
            "Iteration:  67% 795/1191 [01:12<00:36, 10.98it/s]\u001b[A\n",
            "Iteration:  67% 797/1191 [01:12<00:35, 10.98it/s]\u001b[A\n",
            "Iteration:  67% 799/1191 [01:13<00:33, 11.68it/s]\u001b[A\n",
            "Iteration:  67% 801/1191 [01:13<00:36, 10.67it/s]\u001b[A\n",
            "Iteration:  67% 803/1191 [01:13<00:34, 11.27it/s]\u001b[A\n",
            "Iteration:  68% 805/1191 [01:13<00:35, 10.97it/s]\u001b[A\n",
            "Iteration:  68% 807/1191 [01:13<00:35, 10.94it/s]\u001b[A\n",
            "Iteration:  68% 809/1191 [01:13<00:33, 11.53it/s]\u001b[A\n",
            "Iteration:  68% 811/1191 [01:14<00:35, 10.56it/s]\u001b[A\n",
            "Iteration:  68% 813/1191 [01:14<00:33, 11.28it/s]\u001b[A\n",
            "Iteration:  68% 815/1191 [01:14<00:34, 10.91it/s]\u001b[A\n",
            "Iteration:  69% 817/1191 [01:14<00:34, 10.91it/s]\u001b[A\n",
            "Iteration:  69% 819/1191 [01:14<00:32, 11.55it/s]\u001b[A\n",
            "Iteration:  69% 821/1191 [01:15<00:35, 10.51it/s]\u001b[A\n",
            "Iteration:  69% 823/1191 [01:15<00:33, 11.14it/s]\u001b[A\n",
            "Iteration:  69% 825/1191 [01:15<00:33, 10.95it/s]\u001b[A\n",
            "Iteration:  69% 827/1191 [01:15<00:33, 10.85it/s]\u001b[A\n",
            "Iteration:  70% 829/1191 [01:15<00:31, 11.56it/s]\u001b[A\n",
            "Iteration:  70% 831/1191 [01:15<00:33, 10.60it/s]\u001b[A\n",
            "Iteration:  70% 833/1191 [01:16<00:31, 11.32it/s]\u001b[A\n",
            "Iteration:  70% 835/1191 [01:16<00:32, 11.02it/s]\u001b[A\n",
            "Iteration:  70% 837/1191 [01:16<00:32, 11.02it/s]\u001b[A\n",
            "Iteration:  70% 839/1191 [01:16<00:30, 11.64it/s]\u001b[A\n",
            "Iteration:  71% 841/1191 [01:16<00:32, 10.61it/s]\u001b[A\n",
            "Iteration:  71% 843/1191 [01:17<00:30, 11.31it/s]\u001b[A\n",
            "Iteration:  71% 845/1191 [01:17<00:31, 11.04it/s]\u001b[A\n",
            "Iteration:  71% 847/1191 [01:17<00:31, 11.04it/s]\u001b[A\n",
            "Iteration:  71% 849/1191 [01:17<00:29, 11.54it/s]\u001b[A\n",
            "Iteration:  71% 851/1191 [01:17<00:32, 10.51it/s]\u001b[A\n",
            "Iteration:  72% 853/1191 [01:17<00:30, 11.25it/s]\u001b[A\n",
            "Iteration:  72% 855/1191 [01:18<00:30, 10.95it/s]\u001b[A\n",
            "Iteration:  72% 857/1191 [01:18<00:30, 10.93it/s]\u001b[A\n",
            "Iteration:  72% 859/1191 [01:18<00:28, 11.50it/s]\u001b[A\n",
            "Iteration:  72% 861/1191 [01:18<00:31, 10.55it/s]\u001b[A\n",
            "Iteration:  72% 863/1191 [01:18<00:28, 11.32it/s]\u001b[A\n",
            "Iteration:  73% 865/1191 [01:19<00:29, 11.00it/s]\u001b[A\n",
            "Iteration:  73% 867/1191 [01:19<00:29, 11.01it/s]\u001b[A\n",
            "Iteration:  73% 869/1191 [01:19<00:27, 11.58it/s]\u001b[A\n",
            "Iteration:  73% 871/1191 [01:19<00:30, 10.53it/s]\u001b[A\n",
            "Iteration:  73% 873/1191 [01:19<00:28, 11.25it/s]\u001b[A\n",
            "Iteration:  73% 875/1191 [01:19<00:29, 10.89it/s]\u001b[A\n",
            "Iteration:  74% 877/1191 [01:20<00:28, 10.92it/s]\u001b[A\n",
            "Iteration:  74% 879/1191 [01:20<00:27, 11.54it/s]\u001b[A\n",
            "Iteration:  74% 881/1191 [01:20<00:29, 10.58it/s]\u001b[A\n",
            "Iteration:  74% 883/1191 [01:20<00:27, 11.23it/s]\u001b[A\n",
            "Iteration:  74% 885/1191 [01:20<00:27, 10.95it/s]\u001b[A\n",
            "Iteration:  74% 887/1191 [01:21<00:27, 10.98it/s]\u001b[A\n",
            "Iteration:  75% 889/1191 [01:21<00:26, 11.55it/s]\u001b[A\n",
            "Iteration:  75% 891/1191 [01:21<00:28, 10.43it/s]\u001b[A\n",
            "Iteration:  75% 893/1191 [01:21<00:26, 11.12it/s]\u001b[A\n",
            "Iteration:  75% 895/1191 [01:21<00:27, 10.72it/s]\u001b[A\n",
            "Iteration:  75% 897/1191 [01:21<00:27, 10.78it/s]\u001b[A\n",
            "Iteration:  75% 899/1191 [01:22<00:25, 11.51it/s]\u001b[A\n",
            "Iteration:  76% 901/1191 [01:22<00:27, 10.47it/s]\u001b[A\n",
            "Iteration:  76% 903/1191 [01:22<00:25, 11.23it/s]\u001b[A\n",
            "Iteration:  76% 905/1191 [01:22<00:26, 10.88it/s]\u001b[A\n",
            "Iteration:  76% 907/1191 [01:22<00:25, 10.94it/s]\u001b[A\n",
            "Iteration:  76% 909/1191 [01:23<00:24, 11.62it/s]\u001b[A\n",
            "Iteration:  76% 911/1191 [01:23<00:26, 10.61it/s]\u001b[A\n",
            "Iteration:  77% 913/1191 [01:23<00:24, 11.34it/s]\u001b[A\n",
            "Iteration:  77% 915/1191 [01:23<00:25, 10.95it/s]\u001b[A\n",
            "Iteration:  77% 917/1191 [01:23<00:25, 10.91it/s]\u001b[A\n",
            "Iteration:  77% 919/1191 [01:23<00:23, 11.50it/s]\u001b[A\n",
            "Iteration:  77% 921/1191 [01:24<00:25, 10.57it/s]\u001b[A\n",
            "Iteration:  77% 923/1191 [01:24<00:23, 11.27it/s]\u001b[A\n",
            "Iteration:  78% 925/1191 [01:24<00:24, 10.96it/s]\u001b[A\n",
            "Iteration:  78% 927/1191 [01:24<00:24, 10.98it/s]\u001b[A\n",
            "Iteration:  78% 929/1191 [01:24<00:22, 11.56it/s]\u001b[A\n",
            "Iteration:  78% 931/1191 [01:25<00:24, 10.62it/s]\u001b[A\n",
            "Iteration:  78% 933/1191 [01:25<00:22, 11.27it/s]\u001b[A\n",
            "Iteration:  79% 935/1191 [01:25<00:23, 10.90it/s]\u001b[A\n",
            "Iteration:  79% 937/1191 [01:25<00:23, 10.92it/s]\u001b[A\n",
            "Iteration:  79% 939/1191 [01:25<00:22, 11.45it/s]\u001b[A\n",
            "Iteration:  79% 941/1191 [01:25<00:23, 10.53it/s]\u001b[A\n",
            "Iteration:  79% 943/1191 [01:26<00:21, 11.35it/s]\u001b[A\n",
            "Iteration:  79% 945/1191 [01:26<00:22, 11.10it/s]\u001b[A\n",
            "Iteration:  80% 947/1191 [01:26<00:22, 11.04it/s]\u001b[A\n",
            "Iteration:  80% 949/1191 [01:26<00:20, 11.66it/s]\u001b[A\n",
            "Iteration:  80% 951/1191 [01:26<00:22, 10.66it/s]\u001b[A\n",
            "Iteration:  80% 953/1191 [01:27<00:20, 11.39it/s]\u001b[A\n",
            "Iteration:  80% 955/1191 [01:27<00:21, 11.11it/s]\u001b[A\n",
            "Iteration:  80% 957/1191 [01:27<00:21, 11.03it/s]\u001b[A\n",
            "Iteration:  81% 959/1191 [01:27<00:19, 11.74it/s]\u001b[A\n",
            "Iteration:  81% 961/1191 [01:27<00:21, 10.70it/s]\u001b[A\n",
            "Iteration:  81% 963/1191 [01:27<00:20, 11.32it/s]\u001b[A\n",
            "Iteration:  81% 965/1191 [01:28<00:20, 11.02it/s]\u001b[A\n",
            "Iteration:  81% 967/1191 [01:28<00:20, 11.04it/s]\u001b[A\n",
            "Iteration:  81% 969/1191 [01:28<00:19, 11.67it/s]\u001b[A\n",
            "Iteration:  82% 971/1191 [01:28<00:20, 10.56it/s]\u001b[A\n",
            "Iteration:  82% 973/1191 [01:28<00:19, 11.34it/s]\u001b[A\n",
            "Iteration:  82% 975/1191 [01:29<00:19, 10.84it/s]\u001b[A\n",
            "Iteration:  82% 977/1191 [01:29<00:19, 10.88it/s]\u001b[A\n",
            "Iteration:  82% 979/1191 [01:29<00:18, 11.52it/s]\u001b[A\n",
            "Iteration:  82% 981/1191 [01:29<00:19, 10.57it/s]\u001b[A\n",
            "Iteration:  83% 983/1191 [01:29<00:18, 11.37it/s]\u001b[A\n",
            "Iteration:  83% 985/1191 [01:29<00:18, 10.93it/s]\u001b[A\n",
            "Iteration:  83% 987/1191 [01:30<00:18, 10.97it/s]\u001b[A\n",
            "Iteration:  83% 989/1191 [01:30<00:17, 11.68it/s]\u001b[A\n",
            "Iteration:  83% 991/1191 [01:30<00:18, 10.67it/s]\u001b[A\n",
            "Iteration:  83% 993/1191 [01:30<00:17, 11.46it/s]\u001b[A\n",
            "Iteration:  84% 995/1191 [01:30<00:17, 11.16it/s]\u001b[A\n",
            "Iteration:  84% 997/1191 [01:30<00:17, 10.93it/s]\u001b[A\n",
            "Iteration:  84% 999/1191 [01:31<00:16, 11.67it/s]\u001b[A\n",
            "Iteration:  84% 1001/1191 [01:31<00:17, 10.64it/s]\u001b[A\n",
            "Iteration:  84% 1003/1191 [01:31<00:16, 11.43it/s]\u001b[A\n",
            "Iteration:  84% 1005/1191 [01:31<00:16, 11.17it/s]\u001b[A\n",
            "Iteration:  85% 1007/1191 [01:31<00:16, 11.13it/s]\u001b[A\n",
            "Iteration:  85% 1009/1191 [01:32<00:15, 11.75it/s]\u001b[A\n",
            "Iteration:  85% 1011/1191 [01:32<00:16, 10.68it/s]\u001b[A\n",
            "Iteration:  85% 1013/1191 [01:32<00:15, 11.38it/s]\u001b[A\n",
            "Iteration:  85% 1015/1191 [01:32<00:15, 11.12it/s]\u001b[A\n",
            "Iteration:  85% 1017/1191 [01:32<00:15, 11.08it/s]\u001b[A\n",
            "Iteration:  86% 1019/1191 [01:32<00:14, 11.54it/s]\u001b[A\n",
            "Iteration:  86% 1021/1191 [01:33<00:16, 10.57it/s]\u001b[A\n",
            "Iteration:  86% 1023/1191 [01:33<00:14, 11.31it/s]\u001b[A\n",
            "Iteration:  86% 1025/1191 [01:33<00:15, 11.00it/s]\u001b[A\n",
            "Iteration:  86% 1027/1191 [01:33<00:14, 10.96it/s]\u001b[A\n",
            "Iteration:  86% 1029/1191 [01:33<00:14, 11.54it/s]\u001b[A\n",
            "Iteration:  87% 1031/1191 [01:34<00:15, 10.46it/s]\u001b[A\n",
            "Iteration:  87% 1033/1191 [01:34<00:14, 11.27it/s]\u001b[A\n",
            "Iteration:  87% 1035/1191 [01:34<00:14, 10.94it/s]\u001b[A\n",
            "Iteration:  87% 1037/1191 [01:34<00:14, 10.97it/s]\u001b[A\n",
            "Iteration:  87% 1039/1191 [01:34<00:13, 11.58it/s]\u001b[A\n",
            "Iteration:  87% 1041/1191 [01:34<00:14, 10.57it/s]\u001b[A\n",
            "Iteration:  88% 1043/1191 [01:35<00:13, 11.28it/s]\u001b[A\n",
            "Iteration:  88% 1045/1191 [01:35<00:13, 10.93it/s]\u001b[A\n",
            "Iteration:  88% 1047/1191 [01:35<00:13, 10.85it/s]\u001b[A\n",
            "Iteration:  88% 1049/1191 [01:35<00:12, 11.52it/s]\u001b[A\n",
            "Iteration:  88% 1051/1191 [01:35<00:13, 10.47it/s]\u001b[A\n",
            "Iteration:  88% 1053/1191 [01:36<00:12, 11.06it/s]\u001b[A\n",
            "Iteration:  89% 1055/1191 [01:36<00:12, 10.84it/s]\u001b[A\n",
            "Iteration:  89% 1057/1191 [01:36<00:12, 10.92it/s]\u001b[A\n",
            "Iteration:  89% 1059/1191 [01:36<00:11, 11.63it/s]\u001b[A\n",
            "Iteration:  89% 1061/1191 [01:36<00:12, 10.64it/s]\u001b[A\n",
            "Iteration:  89% 1063/1191 [01:36<00:11, 11.39it/s]\u001b[A\n",
            "Iteration:  89% 1065/1191 [01:37<00:11, 10.88it/s]\u001b[A\n",
            "Iteration:  90% 1067/1191 [01:37<00:11, 11.10it/s]\u001b[A\n",
            "Iteration:  90% 1069/1191 [01:37<00:10, 11.65it/s]\u001b[A\n",
            "Iteration:  90% 1071/1191 [01:37<00:11, 10.68it/s]\u001b[A\n",
            "Iteration:  90% 1073/1191 [01:37<00:10, 11.35it/s]\u001b[A\n",
            "Iteration:  90% 1075/1191 [01:38<00:10, 10.86it/s]\u001b[A\n",
            "Iteration:  90% 1077/1191 [01:38<00:10, 11.12it/s]\u001b[A\n",
            "Iteration:  91% 1079/1191 [01:38<00:09, 11.57it/s]\u001b[A\n",
            "Iteration:  91% 1081/1191 [01:38<00:10, 10.59it/s]\u001b[A\n",
            "Iteration:  91% 1083/1191 [01:38<00:09, 11.37it/s]\u001b[A\n",
            "Iteration:  91% 1085/1191 [01:38<00:09, 11.07it/s]\u001b[A\n",
            "Iteration:  91% 1087/1191 [01:39<00:09, 10.91it/s]\u001b[A\n",
            "Iteration:  91% 1089/1191 [01:39<00:08, 11.41it/s]\u001b[A\n",
            "Iteration:  92% 1091/1191 [01:39<00:09, 10.45it/s]\u001b[A\n",
            "Iteration:  92% 1093/1191 [01:39<00:08, 11.25it/s]\u001b[A\n",
            "Iteration:  92% 1095/1191 [01:39<00:08, 10.92it/s]\u001b[A\n",
            "Iteration:  92% 1097/1191 [01:40<00:08, 10.83it/s]\u001b[A\n",
            "Iteration:  92% 1099/1191 [01:40<00:08, 11.37it/s]\u001b[A\n",
            "Iteration:  92% 1101/1191 [01:40<00:08, 10.42it/s]\u001b[A\n",
            "Iteration:  93% 1103/1191 [01:40<00:07, 11.21it/s]\u001b[A\n",
            "Iteration:  93% 1105/1191 [01:40<00:07, 10.96it/s]\u001b[A\n",
            "Iteration:  93% 1107/1191 [01:40<00:07, 10.98it/s]\u001b[A\n",
            "Iteration:  93% 1109/1191 [01:41<00:07, 11.50it/s]\u001b[A\n",
            "Iteration:  93% 1111/1191 [01:41<00:07, 10.53it/s]\u001b[A\n",
            "Iteration:  93% 1113/1191 [01:41<00:07, 11.13it/s]\u001b[A\n",
            "Iteration:  94% 1115/1191 [01:41<00:07, 10.85it/s]\u001b[A\n",
            "Iteration:  94% 1117/1191 [01:41<00:06, 10.90it/s]\u001b[A\n",
            "Iteration:  94% 1119/1191 [01:41<00:06, 11.44it/s]\u001b[A\n",
            "Iteration:  94% 1121/1191 [01:42<00:06, 10.51it/s]\u001b[A\n",
            "Iteration:  94% 1123/1191 [01:42<00:06, 11.24it/s]\u001b[A\n",
            "Iteration:  94% 1125/1191 [01:42<00:06, 10.84it/s]\u001b[A\n",
            "Iteration:  95% 1127/1191 [01:42<00:05, 10.83it/s]\u001b[A\n",
            "Iteration:  95% 1129/1191 [01:42<00:05, 11.49it/s]\u001b[A\n",
            "Iteration:  95% 1131/1191 [01:43<00:05, 10.52it/s]\u001b[A\n",
            "Iteration:  95% 1133/1191 [01:43<00:05, 11.21it/s]\u001b[A\n",
            "Iteration:  95% 1135/1191 [01:43<00:05, 10.93it/s]\u001b[A\n",
            "Iteration:  95% 1137/1191 [01:43<00:04, 10.91it/s]\u001b[A\n",
            "Iteration:  96% 1139/1191 [01:43<00:04, 11.58it/s]\u001b[A\n",
            "Iteration:  96% 1141/1191 [01:44<00:04, 10.61it/s]\u001b[A\n",
            "Iteration:  96% 1143/1191 [01:44<00:04, 11.31it/s]\u001b[A\n",
            "Iteration:  96% 1145/1191 [01:44<00:04, 10.92it/s]\u001b[A\n",
            "Iteration:  96% 1147/1191 [01:44<00:04, 10.85it/s]\u001b[A\n",
            "Iteration:  96% 1149/1191 [01:44<00:03, 11.58it/s]\u001b[A\n",
            "Iteration:  97% 1151/1191 [01:44<00:03, 10.56it/s]\u001b[A\n",
            "Iteration:  97% 1153/1191 [01:45<00:03, 11.28it/s]\u001b[A\n",
            "Iteration:  97% 1155/1191 [01:45<00:03, 10.89it/s]\u001b[A\n",
            "Iteration:  97% 1157/1191 [01:45<00:03, 10.89it/s]\u001b[A\n",
            "Iteration:  97% 1159/1191 [01:45<00:02, 11.53it/s]\u001b[A\n",
            "Iteration:  97% 1161/1191 [01:45<00:02, 10.43it/s]\u001b[A\n",
            "Iteration:  98% 1163/1191 [01:46<00:02, 11.18it/s]\u001b[A\n",
            "Iteration:  98% 1165/1191 [01:46<00:02, 10.86it/s]\u001b[A\n",
            "Iteration:  98% 1167/1191 [01:46<00:02, 10.89it/s]\u001b[A\n",
            "Iteration:  98% 1169/1191 [01:46<00:01, 11.50it/s]\u001b[A\n",
            "Iteration:  98% 1171/1191 [01:46<00:01, 10.51it/s]\u001b[A\n",
            "Iteration:  98% 1173/1191 [01:46<00:01, 11.22it/s]\u001b[A\n",
            "Iteration:  99% 1175/1191 [01:47<00:01, 10.93it/s]\u001b[A\n",
            "Iteration:  99% 1177/1191 [01:47<00:01, 10.84it/s]\u001b[A\n",
            "Iteration:  99% 1179/1191 [01:47<00:01, 11.50it/s]\u001b[A\n",
            "Iteration:  99% 1181/1191 [01:47<00:00, 10.54it/s]\u001b[A\n",
            "Iteration:  99% 1183/1191 [01:47<00:00, 11.12it/s]\u001b[A\n",
            "Iteration:  99% 1185/1191 [01:48<00:00, 10.89it/s]\u001b[A\n",
            "Iteration: 100% 1187/1191 [01:48<00:00, 10.89it/s]\u001b[A\n",
            "Iteration: 100% 1189/1191 [01:48<00:00, 11.56it/s]\u001b[A\n",
            "Iteration: 100% 1191/1191 [01:48<00:00, 10.97it/s]\n",
            "Epoch: 100% 1/1 [01:48<00:00, 108.58s/it]\n",
            "12/29/2021 12:23:08 - INFO - __main__ -    global_step = 238, average loss = 2.3894133030303886\n",
            "12/29/2021 12:23:08 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/finetuned_models/presidential_speeches\n",
            "12/29/2021 12:23:08 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 12:23:11 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/finetuned_models/presidential_speeches/pytorch_model.bin\n",
            "12/29/2021 12:23:11 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 12:23:11 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:23:11 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/presidential_speeches/pytorch_model.bin\n",
            "12/29/2021 12:23:16 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 12:23:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/presidential_speeches' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/presidential_speeches' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/presidential_speeches/added_tokens.json. We won't load it.\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/vocab.json\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/merges.txt\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   loading file None\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/special_tokens_map.json\n",
            "12/29/2021 12:23:16 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/tokenizer_config.json\n",
            "12/29/2021 12:23:16 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/My Drive/finetuned_models/presidential_speeches']\n",
            "12/29/2021 12:23:16 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 12:23:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:23:16 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/presidential_speeches/pytorch_model.bin\n",
            "12/29/2021 12:23:22 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "12/29/2021 12:23:25 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_32_valid.txt\n",
            "12/29/2021 12:23:25 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "12/29/2021 12:23:25 - INFO - __main__ -     Num examples = 18617\n",
            "12/29/2021 12:23:25 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating: 100% 9309/9309 [02:52<00:00, 53.86it/s]\n",
            "12/29/2021 12:26:17 - INFO - __main__ -   ***** Eval results  *****\n",
            "12/29/2021 12:26:17 - INFO - __main__ -     perplexity = tensor(16.6660)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/presidential_speeches'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlo0KUyLIrDy",
        "outputId": "2c1e264d-36dd-425f-9085-14c05959288e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t  merges.txt\t     special_tokens_map.json  training_args.bin\n",
            "eval_results.txt  pytorch_model.bin  tokenizer_config.json    vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None\n",
        "  )\n",
        "  \n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "SpqtrRzZKDES"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "#CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-1500'\n",
        "CHECKPOINT_PATH = \"gpt2\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/valid.txt\",\n",
        "              \"/content/test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = dicttoobject.dict_to_writable_object(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbcT2W6ZKIUY",
        "outputId": "7f3b1c1a-01b6-4f8b-e49f-9451bce1dfb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 12:51:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:51:21 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 12:51:21 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "12/29/2021 12:51:25 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "12/29/2021 12:51:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:51:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 12:51:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 12:51:26 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 12:51:26 - INFO - run_language_modeling -   Loading features from cached file /content/gpt2_cached_lm_128_valid.txt\n",
            "12/29/2021 12:51:26 - INFO - run_language_modeling -   ***** Running evaluation  *****\n",
            "12/29/2021 12:51:26 - INFO - run_language_modeling -     Num examples = 4654\n",
            "12/29/2021 12:51:26 - INFO - run_language_modeling -     Batch size = 2\n",
            "Evaluating: 100%|██████████| 2327/2327 [01:47<00:00, 21.57it/s]\n",
            "12/29/2021 12:53:14 - INFO - run_language_modeling -   ***** Eval results  *****\n",
            "12/29/2021 12:53:14 - INFO - run_language_modeling -     perplexity = tensor(14.4401)\n",
            "12/29/2021 12:53:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 12:53:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.440129280090332 is the perplexity of /content/valid.txt according to gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 12:53:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 12:53:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 12:53:15 - INFO - run_language_modeling -   Loading features from cached file /content/gpt2_cached_lm_128_test.txt\n",
            "12/29/2021 12:53:15 - INFO - run_language_modeling -   ***** Running evaluation  *****\n",
            "12/29/2021 12:53:15 - INFO - run_language_modeling -     Num examples = 4094\n",
            "12/29/2021 12:53:15 - INFO - run_language_modeling -     Batch size = 2\n",
            "Evaluating: 100%|██████████| 2047/2047 [01:35<00:00, 21.43it/s]\n",
            "12/29/2021 12:54:50 - INFO - run_language_modeling -   ***** Eval results  *****\n",
            "12/29/2021 12:54:50 - INFO - run_language_modeling -     perplexity = tensor(15.4939)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.493876457214355 is the perplexity of /content/test.txt according to gpt2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "metadata": {
        "id": "GVjMG1WxKVq-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\n",
        "# to generate with the pre-trained model without finetuning.\n",
        "#CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-1500'\n",
        "CHECKPOINT_PATH = \"gpt2\"\n",
        "# You should try out other prompts as well as no prompt at all.\n",
        "PROMPT = '<title=\\\"How are you rich guy\\\">'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        "  stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "  temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "  k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "  p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "  repetition_penalty=None,\n",
        "  length=100,  # Number of tokens to generate.\n",
        "  num_return_sequences=3,  # Number of independently computed samples to generate.\n",
        ")\n",
        "args = dicttoobject.dict_to_writable_object(args)\n",
        "\n",
        "model = load_model(args)\n",
        "sequences = generate_samples(args, model, PROMPT)\n",
        "for idx, sequence in enumerate(sequences):\n",
        "  print('\\n====== GENERATION {} ======'.format(idx))\n",
        "  print(sequence)"
      ],
      "metadata": {
        "id": "jFWt6QM9Lw0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3904f83-5610-485c-ecad-71d20297ae4c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 13:32:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 13:32:21 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 13:32:22 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "12/29/2021 13:32:26 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "12/29/2021 13:32:27 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "12/29/2021 13:32:27 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 13:32:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "12/29/2021 13:32:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "12/29/2021 13:32:27 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "<title=\"How are you rich guy\">You're rich!!</a>.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "<title=\"How are you rich guy\">How?</title>\n",
            "\n",
            "#endregion\n",
            "\n",
            "<!DOCTYPE html> <title>Newly-Added - All You Need to Know</title>\n",
            "\n",
            "#endregion\n",
            "\n",
            "<!DOCTYPE html> <title>Newly-Added - A Game to Play Here</title>\n",
            "\n",
            "#endregion\n",
            "\n",
            "</select> <title>Favourite Game</title> <filter> <numberOfBugs>1</numberOf\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "<title=\"How are you rich guy\">I am rich, but I get very little. {{TitleText} says if you are rich then your children, but don't count on it.\n",
            "\n",
            "<script src=\"gives_questions.js\"></script>\n",
            "\n",
            "But do you think, \"This makes sense as a single sentence?\" and think, \"I think this is the best way of getting rich?\" Yes, by talking.\n",
            "\n",
            "Why do you think you are rich?\n",
            "\n",
            "You don't know what you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/gpt2/'\n",
        "#CHECKPOINT_PATH = \"gpt2\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/valid.txt\",\n",
        "              \"/content/test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = dicttoobject.dict_to_writable_object(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "2alaj1CpNZTB",
        "outputId": "085528a2-2612-492d-c946-6df15fc6ddaa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a407505dc58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicttoobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_to_writable_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATA_PATHS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-5969f7fdacd7>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;34m\"\"\"Creates a model and loads in weights for it.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   model = AutoModelWithLMHead.from_pretrained(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \"\"\"\n\u001b[1;32m    186\u001b[0m         config_dict, _ = PretrainedConfig.get_config_dict(\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     )\n\u001b[1;32m    272\u001b[0m                 )\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load '/content/drive/My Drive/finetuned_models/presidential_speeches/gpt2/'. Make sure that:\n\n- '/content/drive/My Drive/finetuned_models/presidential_speeches/gpt2/' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/content/drive/My Drive/finetuned_models/presidential_speeches/gpt2/' is the correct path to a directory containing a 'config.json' file\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\n",
        "# to generate with the pre-trained model without finetuning.\n",
        "#CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/checkpoint-1500'\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/presidential_speeches/'\n",
        "# You should try out other prompts as well as no prompt at all.\n",
        "PROMPT = '<title=\\\"How are you poor guy\\\">'\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        "  stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "  temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "  k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "  p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "  repetition_penalty=None,\n",
        "  length=100,  # Number of tokens to generate.\n",
        "  num_return_sequences=3,  # Number of independently computed samples to generate.\n",
        ")\n",
        "args = dicttoobject.dict_to_writable_object(args)\n",
        "\n",
        "model = load_model(args)\n",
        "sequences = generate_samples(args, model, PROMPT)\n",
        "for idx, sequence in enumerate(sequences):\n",
        "  print('\\n====== GENERATION {} ======'.format(idx))\n",
        "  print(sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7K8uWaBRYVQ",
        "outputId": "518cb0e4-065f-496b-c767-96d53d729b30"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 13:32:46 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 13:32:46 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 13:32:46 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/presidential_speeches/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/29/2021 13:32:52 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/presidential_speeches/config.json\n",
            "12/29/2021 13:32:52 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/presidential_speeches/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/presidential_speeches/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/presidential_speeches/added_tokens.json. We won't load it.\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/vocab.json\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/merges.txt\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   loading file None\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/special_tokens_map.json\n",
            "12/29/2021 13:32:52 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/presidential_speeches/tokenizer_config.json\n",
            "12/29/2021 13:32:52 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "<title=\"How are you poor guy\">\n",
            "    <ul class=\"container js-lg-select js-lg-label menu-ht mr-0 mr-3\"><li class=\"selected-lg-element d-flex flex-wrap flex-wrap-lg-hidden flex-hydro-lg-justify-click-hmac js-mac-click-hydro-hmac\">\n",
            "    <li class=\"col-md-2 flex-left\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "<title=\"How are you poor guy\">How?</span>\n",
            "    </span>\n",
            "         /span>\n",
            "          <link rel=\"icon\" href=\"/interactive-fiction-class/interactive-fiction-class.github.io/chunk-template\" data-src=\"https://github.com/interactive-fiction-class/interactive-fiction-class.github.io\" data-target=\"index.github\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "<title=\"How are you poor guy\">Education</option>\n",
            "\n",
            "       <meta name=\"css/chunk-content/chunk-content;:click.include_on_link.js\" integrity=\"sha512-9ZjN2G1V5jKnVdVUkqfZjK7dzLxwBNpjw+9ZQXcQ1o==\" content=\"Select your color, tab, button, text input in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WRGVh3D2Su4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}